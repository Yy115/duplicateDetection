{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4b27da2-abaf-4203-ac50-cd24176c0d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (4.44.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from transformers) (0.24.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from requests->transformers) (2024.12.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febed13c-32d5-4a51-a26c-7849803f354e",
   "metadata": {},
   "source": [
    "\"\"\"##**Importing the tools**\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d26d3afb-6218-471e-a12f-56e7177781d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch\n",
    "import transformers as ppb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "import re\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9d289fc-f0b4-40a5-80cf-a4d19317abac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thunderbird\n",
    "df1=pd.read_csv(r'https://raw.githubusercontent.com/Yy115/DupliacateDetectionDataset/refs/heads/main/ThunderBird/dup_TB.csv',delimiter=';')\n",
    "df2=pd.read_csv(r'https://raw.githubusercontent.com/Yy115/DupliacateDetectionDataset/refs/heads/main/ThunderBird/Nondup_TB.csv',delimiter=';')\n",
    "\n",
    "df1['Label'] = 'duplicate'\n",
    "df2['Label'] = 'non duplicate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dd292f9-e7e5-47a6-9c62-9bebf7448230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Issue_id  Duplicated_issue  \\\n",
      "0    209996            207508   \n",
      "1    214111            448288   \n",
      "2    214126            213406   \n",
      "3    214354            217255   \n",
      "4    214358            213212   \n",
      "\n",
      "                                              Title1  \\\n",
      "0  mail window shakes when one email is in a map ...   \n",
      "1  drag and drop only drags the top message from ...   \n",
      "2            cannot launch firebird from thunderbird   \n",
      "3               crash starting thunderbird  on linux   \n",
      "4  when clicking down scroll arrow in contacts si...   \n",
      "\n",
      "                                        Description1  \\\n",
      "0    the main widow of thunderbird goes all shaky...   \n",
      "1    drag and drop used for move only drags the t...   \n",
      "2    when click linklocation on message paneerror...   \n",
      "3   optmozillathunderbirdthunderbird  thunderbird...   \n",
      "4    it is unclear to me whether ts bug is part o...   \n",
      "\n",
      "                                              Title2  \\\n",
      "0  sent mail box screen shakes up and down  even ...   \n",
      "1                 group actions on collapsed threads   \n",
      "2  can not invoke mozilla firebird to do web surf...   \n",
      "3  crash with message gdkwarning  gdkdrawablexc d...   \n",
      "4  double clicking scrollbar updown arrows in con...   \n",
      "\n",
      "                                        Description2      Label  \n",
      "0    not sure why but when i open my sent mail bo...  duplicate  \n",
      "1   group actions  v  when a thread is collapsed ...  duplicate  \n",
      "2    probably ts is not the place to report ts in...  duplicate  \n",
      "3    process started but before display opened it...  duplicate  \n",
      "4    in thunderbird click write to start new mail...  duplicate  \n"
     ]
    }
   ],
   "source": [
    "print(df1.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10f1b6c-2581-44d2-9f45-1e2a7da169f2",
   "metadata": {},
   "source": [
    "#### \\反斜杠可能会被认为转义，在绝对路径之前加上r以表示非转义，来解决SyntaxError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "145b8bfb-a8ce-4a5c-9885-4d4146e3cb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"##**Loading the Pre-trained BERT model**\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84bed39a-265d-46f6-8170-96a480a65d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# **Remove stop words**\"\"\"\n",
    "\n",
    "df1['Title1']= df1['Title1'].str.replace(r'i' 'me' 'my' 'myself' 'we' \n",
    "                                         'our' 'ours' 'ourselves' 'you' 'your' 'yours' 'yourself''yourselves' 'they' 'we' 'him' 'he' 'him' 'his' 'himself' 'she' 'her' 'hers' 'herself' 'it''its' 'itself' 'they' 'them' 'their' 'theirs' 'themselves'  'what' 'which' 'who' 'whom' 'this' 'that'\n",
    "                                         'these' 'those' 'am' 'is' 'are' 'was' 'were' 'be' 'been' 'being' 'have' 'has' 'had' 'having' 'do' 'does' 'did' 'doing' 'a' 'an' 'the' 'and' 'but' 'if' 'or' 'because' 'as' 'until' 'while' 'of' 'at' 'by' 'for' 'with' 'about' 'against' 'between' 'into' 'through' \n",
    "                                         'during' 'before' 'after' 'above' 'below' 'to' 'from' 'up' 'down' 'in' 'out' 'on' 'off' 'over' 'under' 'again' 'further' 'then' 'once'  'here' 'there' 'when' 'where' 'why' 'how' 'all' 'any' 'both' 'each' 'few' 'more' 'most' 'other' 'some' 'such' 'no' 'nor' 'not' 'only' 'own' 'same' 'so' 'than' 'too' 'very' 's' 't' 'can' 'will' 'just' 'don' 'should' 'now'\n",
    "                                         'java' 'com' 'org' ,'')\n",
    "\n",
    "\n",
    "df1['Title2']= df1['Title2'].str.replace(r'i' 'me' 'my' 'myself' 'we' \n",
    "                                         'our' 'ours' 'ourselves' 'you' 'your' 'yours' 'yourself''yourselves' 'they' 'we' 'him' 'he' 'him' 'his' 'himself' 'she' 'her' 'hers' 'herself' 'it''its' 'itself' 'they' 'them' 'their' 'theirs' 'themselves'  'what' 'which' 'who' 'whom' 'this' 'that'\n",
    "                                         'these' 'those' 'am' 'is' 'are' 'was' 'were' 'be' 'been' 'being' 'have' 'has' 'had' 'having' 'do' 'does' 'did' 'doing' 'a' 'an'\n",
    "                                         'the' 'and' 'but' 'if' 'or' 'because' 'as' 'until' 'while' 'of' 'at' 'by' 'for' 'with' 'about' 'against' 'between' 'into' 'through' 'during' 'before' 'after' 'above' 'below' 'to' 'from' 'up' 'down' 'in' 'out' 'on' 'off' 'over' 'under' 'again' 'further' 'then' 'once' 'here' 'there' 'when' 'where' 'why' 'how' 'all' 'any' 'both' 'each' 'few' 'more' 'most' 'other' 'some' 'such' 'no' 'nor' 'not' 'only' 'own' \n",
    "                                         'same' 'so' 'than' 'too' 'very' 's' 't' 'can' 'will' 'just' 'don' 'should' 'now'\n",
    "                                         'java' 'com' 'org' ,'')\n",
    "\n",
    "df2['Title1']= df2['Title1'].str.replace(r'i' 'me' 'my' 'myself' 'we' 'our' 'ours' 'ourselves' 'you' 'your' 'yours' 'yourself''yourselves' 'they' 'we' 'him' 'he' 'him' 'his' 'himself' 'she'\n",
    "                                         'her' 'hers' 'herself' 'it''its' 'itself' 'they' 'them' 'their' 'theirs' 'themselves'  'what' 'which' 'who' 'whom' 'this' 'that' 'these' 'those' 'am' 'is' 'are' 'was' 'were' 'be' 'been' 'being' 'have' 'has' 'had' 'having' 'do' 'does' 'did' 'doing' 'a' 'an'\n",
    "                                         'the' 'and' 'but' 'if' 'or' 'because' 'as' 'until' 'while' 'of' 'at' 'by' 'for' 'with' 'about' 'against' 'between' 'into' 'through' \n",
    "                                         'during' 'before' 'after' 'above' 'below' 'to' 'from' 'up' 'down' 'in' 'out' 'on' 'off' 'over' 'under' 'again' 'further' 'then' 'once' 'here' 'there' 'when' 'where' 'why' 'how' 'all' 'any' 'both' 'each' 'few' 'more' 'most' 'other' 'some' 'such' 'no' 'nor' 'not' 'only' 'own' 'same' 'so' 'than' 'too' 'very' 's' 't' 'can' 'will' 'just' 'don' 'should' 'now'\n",
    "                                         'java' 'com' 'org' ,'')\n",
    "\n",
    "\n",
    "df2['Title2']= df2['Title2'].str.replace(r'i' 'me' 'my' 'myself' 'we' 'our' 'ours' 'ourselves' 'you' 'your' 'yours' 'yourself''yourselves' 'they' 'we' 'him' 'he' 'him' 'his' 'himself' 'she'\n",
    "                                         'her' 'hers' 'herself' 'it''its' 'itself' 'they' 'them' 'their' 'theirs' 'themselves'  'what' 'which' 'who' 'whom' 'this' 'that' 'these' 'those' 'am' 'is' 'are' 'was' 'were' 'be' 'been' 'being' 'have' 'has' 'had' 'having' 'do' 'does' 'did' 'doing' 'a' 'an'\n",
    "                                         'the' 'and' 'but' 'if' 'or' 'because' 'as' 'until' 'while' 'of' 'at' 'by' 'for' 'with' 'about' 'against' 'between' 'into' 'through' \n",
    "                                         'during' 'before' 'after' 'above' 'below' 'to' 'from' 'up' 'down' 'in' 'out' 'on' 'off' 'over' 'under' 'again' 'further' 'then' 'once' 'here' 'there' 'when' 'where' 'why' 'how' 'all' 'any' 'both' 'each' 'few' 'more' 'most' 'other' 'some' 'such' 'no' 'nor' 'not' 'only' 'own' \n",
    "                                         'same' 'so' 'than' 'too' 'very' 's' 't' 'can' 'will' 'just' 'don' 'should' 'now'\n",
    "                                         'java' 'com' 'org' ,'')\n",
    "\n",
    "df1['Description1']= df1['Description1'].str.replace(r'i' 'me' 'my' 'myself' 'we'  'our' 'ours' 'ourselves' 'you' 'your' 'yours' 'yourself''yourselves' 'they' 'we' 'him' 'he' 'him' 'his' 'himself' 'she'\n",
    "                                         'her' 'hers' 'herself' 'it''its' 'itself' 'they' 'them' 'their' 'theirs' 'themselves'  'what' 'which' 'who' 'whom' 'this' 'that' 'these' 'those' 'am' 'is' 'are' 'was' 'were' 'be' 'been' 'being' 'have' 'has' 'had' 'having' 'do' 'does' 'did' 'doing' 'a' 'an' 'the' 'and' 'but' 'if' 'or' 'because' 'as' 'until' 'while' 'of' 'at' 'by' 'for' 'with' 'about' 'against' 'between' 'into' 'through' \n",
    "                                         'during' 'before' 'after' 'above' 'below' 'to' 'from' 'up' 'down' 'in' 'out' 'on' 'off' 'over' 'under' 'again' 'further' 'then' 'once' \n",
    "                                         'here' 'there' 'when' 'where' 'why' 'how' 'all' 'any' 'both' 'each' 'few' 'more' 'most' 'other' 'some' 'such' 'no' 'nor' 'not' 'only' 'own' 'same' 'so' 'than' 'too' 'very' 's' 't' 'can' 'will' 'just' 'don' 'should' 'now'\n",
    "                                         'java' 'com' 'org' ,'')\n",
    "\n",
    "df1['Description2']= df1['Description2'].str.replace(r'i' 'me' 'my' 'myself' 'we' 'our' 'ours' 'ourselves' 'you' 'your' 'yours' 'yourself''yourselves' 'they' 'we' 'him' 'he' 'him' 'his' 'himself' 'she'\n",
    "                                         'her' 'hers' 'herself' 'it''its' 'itself' 'they' 'them' 'their' 'theirs' 'themselves'  'what' 'which' 'who' 'whom' 'this' 'that' 'these' 'those' 'am' 'is' 'are' 'was' 'were' 'be' 'been' 'being' 'have' 'has' 'had' 'having' 'do' 'does' 'did' 'doing' 'a' 'an'\n",
    "                                         'the' 'and' 'but' 'if' 'or' 'because' 'as' 'until' 'while' 'of' 'at' 'by' 'for' 'with' 'about' 'against' 'between' 'into' 'through' \n",
    "                                         'during' 'before' 'after' 'above' 'below' 'to' 'from' 'up' 'down' 'in' 'out' 'on' 'off' 'over' 'under' 'again' 'further' 'then' 'once' 'here' 'there' 'when' 'where' 'why' 'how' 'all' 'any' 'both' 'each' 'few' 'more' 'most' 'other' 'some' 'such' 'no' 'nor' 'not' 'only' 'own' \n",
    "                                         'same' 'so' 'than' 'too' 'very' 's' 't' 'can' 'will' 'just' 'don' 'should' 'now'\n",
    "                                         'java' 'com' 'org' ,'')\n",
    "\n",
    "df2['Description1']= df2['Description1'].str.replace(r'i' 'me' 'my' 'myself' 'we' 'our' 'ours' 'ourselves' 'you' 'your' 'yours' 'yourself''yourselves' 'they' 'we' 'him' 'he' 'him' 'his' 'himself' 'she'\n",
    "                                         'her' 'hers' 'herself' 'it''its' 'itself' 'they' 'them' 'their' 'theirs' 'themselves'  'what' 'which' 'who' 'whom' 'this' 'that' 'these' 'those' 'am' 'is' 'are' 'was' 'were' 'be' 'been' 'being' 'have' 'has' 'had' 'having' 'do' 'does' 'did' 'doing' 'a' 'an'\n",
    "                                         'the' 'and' 'but' 'if' 'or' 'because' 'as' 'until' 'while' 'of' 'at' 'by' 'for' 'with' 'about' 'against' 'between' 'into' 'through' 'during' 'before' 'after' 'above' 'below' 'to' 'from' 'up' 'down' 'in' 'out' 'on' 'off' 'over' 'under' 'again' 'further' 'then' 'once' \n",
    "                                         'here' 'there' 'when' 'where' 'why' 'how' 'all' 'any' 'both' 'each' 'few' 'more' 'most' 'other' 'some' 'such' 'no' 'nor' 'not' 'only' 'own' 'same' 'so' 'than' 'too' 'very' 's' 't' 'can' 'will' 'just' 'don' 'should' 'now'\n",
    "                                         'java' 'com' 'org' ,'')\n",
    "\n",
    "df2['Description2']= df2['Description2'].str.replace(r'i' 'me' 'my' 'myself' 'we' 'our' 'ours' 'ourselves' 'you' 'your' 'yours' 'yourself''yourselves' 'they' 'we' 'him' 'he' 'him' 'his' 'himself' 'she'\n",
    "                                         'her' 'hers' 'herself' 'it''its' 'itself' 'they' 'them' 'their' 'theirs' 'themselves'  'what' 'which' 'who' 'whom' 'this' 'that' 'these' 'those' 'am' 'is' 'are' 'was' 'were' 'be' 'been' 'being' 'have' 'has' 'had' 'having' 'do' 'does' 'did' 'doing' 'a' 'an'\n",
    "                                         'the' 'and' 'but' 'if' 'or' 'because' 'as' 'until' 'while' 'of' 'at' 'by' 'for' 'with' 'about' 'against' 'between' 'into' 'through' 'during' 'before' 'after' 'above' 'below' 'to' 'from' 'up' 'down' 'in' 'out' 'on' 'off' 'over' 'under' 'again' 'further' 'then' 'once' \n",
    "                                         'here' 'there' 'when' 'where' 'why' 'how' 'all' 'any' 'both' 'each' 'few' 'more' 'most' 'other' 'some' 'such' 'no' 'nor' 'not' 'only' 'own'  'same' 'so' 'than' 'too' 'very' 's' 't' 'can' 'will' 'just' 'don' 'should' 'now'\n",
    "                                         'java' 'com' 'org' ,'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d5a7ad2-856d-4220-85c2-57e5cdc02a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#**Batch ThunderBird**\"\"\"\n",
    "\n",
    "batch_31=df1[:500]\n",
    "batch_32=df2[:500]\n",
    "df3 = pd.concat([batch_31,batch_32], ignore_index=True)\n",
    "batch_41=df1[500:1000]\n",
    "batch_42=df2[500:1000]\n",
    "df4 = pd.concat([batch_41,batch_42], ignore_index=True)\n",
    "batch_51=df1[1000:1500]\n",
    "batch_52=df2[1000:1500]\n",
    "df5 = pd.concat([batch_51,batch_52], ignore_index=True)\n",
    "batch_61=df1[1500:2000]\n",
    "batch_62=df2[1500:2000]\n",
    "df6 = pd.concat([batch_61,batch_62], ignore_index=True)\n",
    "batch_71=df1[2000:2500]\n",
    "batch_72=df2[2000:2500]\n",
    "df7 = pd.concat([batch_71,batch_72], ignore_index=True)\n",
    "batch_81=df1[2500:3000]\n",
    "batch_82=df2[2500:3000]\n",
    "df8 = pd.concat([batch_81,batch_82], ignore_index=True)\n",
    "batch_91=df1[3000:3486]\n",
    "batch_92=df2[3000:3486]\n",
    "df9 = pd.concat([batch_91,batch_92], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9edda4d8-ab81-424e-af98-1f8096d64052",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing\n",
    "batch_101=df1[3486:3900]\n",
    "batch_102=df2[3486:3900]\n",
    "df10 = pd.concat([batch_101,batch_102], ignore_index=True)\n",
    "batch_111=df1[3900:4338]\n",
    "batch_112=df2[3900:4374]\n",
    "df11 = pd.concat([batch_111,batch_112], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877326a4-c18b-41e9-801d-0b18599992a8",
   "metadata": {},
   "source": [
    "### **_get_segments3**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1201a414-0b75-402d-a80d-ef69e1ede779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_segments3(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    segments = []\n",
    "    first_sep = False\n",
    "    current_segment_id = 0 \n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        #print(token)\n",
    "        if token == 102:\n",
    "            #if first_sep:\n",
    "                #first_sep = False \n",
    "            #else:\n",
    "           current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553d7d35-be59-4062-9c5f-0ca6a12f1afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#**df3**\"\"\"\n",
    "\n",
    "pair3= df3['Title1'] + df3['Description1']+ [\" [SEP] \"] + df3['Title2'] + df3['Description2']\n",
    "tokenized3 = pair3.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))\n",
    "\n",
    "max_len3 = 0                 # padding all lists to the same size\n",
    "for i in tokenized3.values:\n",
    "    if len(i) > max_len3:\n",
    "        max_len3 = len(i)\n",
    "max_len3 =300\n",
    "padded3 = np.array([i + [0]*(max_len3-len(i)) for i in tokenized3.values])\n",
    "\n",
    "np.array(padded3).shape\n",
    "\n",
    "attention_mask3 = np.where(padded3 != 0, 1, 0)\n",
    "attention_mask3.shape\n",
    "input_ids3 = torch.tensor(padded3)  \n",
    "attention_mask3 = torch.tensor(attention_mask3)\n",
    "input_segments3= np.array([_get_segments3(token, max_len3)for token in tokenized3.values])\n",
    "token_type_ids3 = torch.tensor(input_segments3)\n",
    "input_segments3 = torch.tensor(input_segments3)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states3 = model(input_ids3, attention_mask=attention_mask3, token_type_ids=input_segments3)    # <<< 600 rows only !!!\n",
    "features3 = last_hidden_states3[0][:,0,:].numpy()\n",
    "features3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96735e25-4041-4bc7-be8f-711209c67d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#**df4**\"\"\"\n",
    "\n",
    "pair4=df4['Title1'] + df4['Description1']+ [\" [SEP] \"] + df4['Title2']  + df4['Description2']\n",
    "tokenized4 = pair4.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))\n",
    "max_len4 = 0                 # padding all lists to the same size\n",
    "for i in tokenized4.values:\n",
    "    if len(i) > max_len4:\n",
    "        max_len4 = len(i)\n",
    "max_len4 =300\n",
    "padded4 = np.array([i + [0]*(max_len4-len(i)) for i in tokenized4.values])\n",
    "\n",
    "np.array(padded4).shape\n",
    "\n",
    "attention_mask4 = np.where(padded4 != 0, 1, 0)\n",
    "attention_mask4.shape\n",
    "input_ids4 = torch.tensor(padded4)  \n",
    "attention_mask4 = torch.tensor(attention_mask4)\n",
    "input_segments4= np.array([_get_segments3(token, max_len4)for token in tokenized4.values])\n",
    "token_type_ids4 = torch.tensor(input_segments4)\n",
    "input_segments4 = torch.tensor(input_segments4)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states4 = model(input_ids4, attention_mask=attention_mask4, token_type_ids=input_segments4)   \n",
    "features4 = last_hidden_states4[0][:,0,:].numpy()\n",
    "features4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026a2704-5621-4630-b938-e551f748a6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#**df5**\"\"\"\n",
    "\n",
    "pair5=df5['Title1'] + df5['Description1']+ [\" [SEP] \"] + df5['Title2'] + df5['Description2']\n",
    "tokenized5 = pair5.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d150551b-984c-433e-9326-412a74ee5b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"##**Padding**\"\"\"\n",
    "\n",
    "max_len5 = 0                 # padding all lists to the same size\n",
    "for i in tokenized5.values:\n",
    "    if len(i) > max_len5:\n",
    "        max_len5 = len(i)\n",
    "\n",
    "max_len5 =300\n",
    "padded5 = np.array([i + [0]*(max_len5-len(i)) for i in tokenized5.values])\n",
    "\n",
    "np.array(padded5).shape        # Dimensions of the padded variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a908a88c-23dd-408e-9694-d3f01d5830d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"##**Masking**\"\"\"\n",
    "\n",
    "attention_mask5 = np.where(padded5 != 0, 1, 0)\n",
    "attention_mask5.shape\n",
    "input_ids5 = torch.tensor(padded5)  \n",
    "attention_mask5 = torch.tensor(attention_mask5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaddaed-61ad-41e3-bf6d-7dc3cfd8418d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##**Running the `model()` function through BERT**\"\"\"\n",
    "\n",
    "input_segments5= np.array([_get_segments3(token, max_len5)for token in tokenized5.values])\n",
    "token_type_ids5 = torch.tensor(input_segments5)\n",
    "input_segments5 = torch.tensor(input_segments5)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states5 = model(input_ids5, attention_mask=attention_mask5, token_type_ids=input_segments5)    # <<< 600 rows only !!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213ea7e4-dbe8-41af-9c83-15ef737681b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"##**Slicing the part of the output of BERT : [cls]**\"\"\"\n",
    "\n",
    "features5 = last_hidden_states5[0][:,0,:].numpy()\n",
    "features5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5014378e-71c9-4952-ba98-7329d10eda45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#**df6**\"\"\"\n",
    "\n",
    "pair6=df6['Title1'] + df6['Description1']+ [\" [SEP] \"] + df6['Title2'] + df6['Description2']\n",
    "tokenized6 = pair6.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))\n",
    "\n",
    "max_len6 = 0                 # padding all lists to the same size\n",
    "for i in tokenized6.values:\n",
    "    if len(i) > max_len6:\n",
    "        max_len6 = len(i)\n",
    "\n",
    "max_len6=300\n",
    "padded6 = np.array([i + [0]*(max_len6-len(i)) for i in tokenized6.values])\n",
    "\n",
    "np.array(padded6).shape        # Dimensions of the padded variable        \n",
    "\n",
    "attention_mask6 = np.where(padded6 != 0, 1, 0)\n",
    "attention_mask6.shape\n",
    "input_ids6 = torch.tensor(padded6)  \n",
    "attention_mask6 = torch.tensor(attention_mask6)\n",
    "input_segments6= np.array([_get_segments3(token, max_len6)for token in tokenized6.values])\n",
    "token_type_ids6 = torch.tensor(input_segments6)\n",
    "input_segments6 = torch.tensor(input_segments6)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states6 = model(input_ids6, attention_mask=attention_mask6, token_type_ids=input_segments6)   \n",
    "features6 = last_hidden_states6[0][:,0,:].numpy()\n",
    "features6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c70405-1dae-4e21-b203-bdff15ec2173",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#**df7**\"\"\"\n",
    "\n",
    "pair7=df7['Title1'] + df7['Description1']+ [\" [SEP] \"] + df7['Title2'] + df7['Description2']\n",
    "tokenized7 = pair7.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))\n",
    "\n",
    "max_len7 = 0                 # padding all lists to the same size\n",
    "for i in tokenized7.values:\n",
    "    if len(i) > max_len7:\n",
    "        max_len7 = len(i)\n",
    "\n",
    "max_len7=300\n",
    "padded7 = np.array([i + [0]*(max_len7-len(i)) for i in tokenized7.values])\n",
    "\n",
    "np.array(padded7).shape        # Dimensions of the padded variable    \n",
    "\n",
    "attention_mask7 = np.where(padded7 != 0, 1, 0)\n",
    "attention_mask7.shape\n",
    "input_ids7 = torch.tensor(padded7)  \n",
    "attention_mask7 = torch.tensor(attention_mask7)\n",
    "input_segments7= np.array([_get_segments3(token, max_len7)for token in tokenized7.values])\n",
    "token_type_ids7 = torch.tensor(input_segments7)\n",
    "input_segments7 = torch.tensor(input_segments7)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states7 = model(input_ids7, attention_mask=attention_mask7, token_type_ids=input_segments7)  \n",
    "features7 = last_hidden_states7[0][:,0,:].numpy()\n",
    "features7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66e5d3f-b5d1-43ac-8250-c16c86c588b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#**df8**\"\"\"\n",
    "\n",
    "pair8=df8['Title1'] + df8['Description1']+ [\" [SEP] \"] + df8['Title2'] + df8['Description2']\n",
    "tokenized8 = pair8.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))\n",
    "\n",
    "max_len8 = 0                 # padding all lists to the same size\n",
    "for i in tokenized8.values:\n",
    "    if len(i) > max_len8:\n",
    "        max_len8 = len(i)\n",
    "max_len8=300\n",
    "padded8 = np.array([i + [0]*(max_len8-len(i)) for i in tokenized8.values])\n",
    "\n",
    "np.array(padded8).shape        # Dimensions of the padded variable  \n",
    "\n",
    "\n",
    "attention_mask8 = np.where(padded8 != 0, 1, 0)\n",
    "attention_mask8.shape\n",
    "input_ids8 = torch.tensor(padded8)  \n",
    "attention_mask8 = torch.tensor(attention_mask8)\n",
    "input_segments8= np.array([_get_segments3(token, max_len8)for token in tokenized8.values])\n",
    "token_type_ids8 = torch.tensor(input_segments8)\n",
    "input_segments8 = torch.tensor(input_segments8)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states8 = model(input_ids8, attention_mask=attention_mask8, token_type_ids=input_segments8)   \n",
    "features8 = last_hidden_states8[0][:,0,:].numpy()\n",
    "features8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc5184d-8cbc-4986-828b-c528ef206f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#**df9**\"\"\"\n",
    "\n",
    "pair9=df9['Title1'] + df9['Description1']+ [\" [SEP] \"] + df9['Title2'] + df9['Description2']\n",
    "tokenized9 = pair9.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))\n",
    "\n",
    "max_len9 = 0                 # padding all lists to the same size\n",
    "for i in tokenized9.values:\n",
    "    if len(i) > max_len9:\n",
    "        max_len9 = len(i)\n",
    "max_len9=300\n",
    "padded9 = np.array([i + [0]*(max_len9-len(i)) for i in tokenized9.values])\n",
    "\n",
    "np.array(padded9).shape        # Dimensions of the padded variable    \n",
    "\n",
    "attention_mask9 = np.where(padded9 != 0, 1, 0)\n",
    "attention_mask9.shape\n",
    "input_ids9 = torch.tensor(padded9)  \n",
    "attention_mask9 = torch.tensor(attention_mask9)\n",
    "input_segments9= np.array([_get_segments3(token, max_len9)for token in tokenized9.values])\n",
    "token_type_ids9 = torch.tensor(input_segments9)\n",
    "input_segments9 = torch.tensor(input_segments9)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states9 = model(input_ids9, attention_mask=attention_mask9, token_type_ids=input_segments9)    \n",
    "features9 = last_hidden_states9[0][:,0,:].numpy()\n",
    "features9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560f0b3c-f716-4843-9f80-411bfb207bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#**df10**\"\"\"\n",
    "\n",
    "pair10=df10['Title1'] + df10['Description1']+ [\" [SEP] \"] + df10['Title2'] + df10['Description2']\n",
    "tokenized10 = pair10.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))\n",
    "max_len10 = 0                 # padding all lists to the same size\n",
    "for i in tokenized10.values:\n",
    "    if len(i) > max_len10:\n",
    "        max_len10 = len(i)\n",
    "max_len10=300\n",
    "padded10 = np.array([i + [0]*(max_len10-len(i)) for i in tokenized10.values])\n",
    "\n",
    "np.array(padded10).shape        # Dimensions of the padded variable\n",
    "\n",
    "attention_mask10 = np.where(padded10 != 0, 1, 0)\n",
    "attention_mask10.shape\n",
    "input_ids10 = torch.tensor(padded10)  \n",
    "attention_mask10 = torch.tensor(attention_mask10)\n",
    "input_segments10= np.array([_get_segments3(token, max_len10)for token in tokenized10.values])\n",
    "token_type_ids10 = torch.tensor(input_segments10)\n",
    "input_segments10 = torch.tensor(input_segments10)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states10 = model(input_ids10, attention_mask=attention_mask10, token_type_ids=input_segments10) \n",
    "features10 = last_hidden_states10[0][:,0,:].numpy()\n",
    "features10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd35387d-e1ce-45e1-b0d2-251f62a23132",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#**df11**\"\"\"\n",
    "\n",
    "pair11=df11['Title1'] + df11['Description1']+ [\" [SEP] \"] + df11['Title2'] + df11['Description2']\n",
    "tokenized11 = pair11.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))\n",
    "\n",
    "max_len11 = 0                 # padding all lists to the same size\n",
    "for i in tokenized11.values:\n",
    "    if len(i) > max_len11:\n",
    "        max_len11 = len(i)\n",
    "max_len11=300\n",
    "padded11 = np.array([i + [0]*(max_len11-len(i)) for i in tokenized11.values])\n",
    "\n",
    "np.array(padded11).shape        # Dimensions of the padded variable   \n",
    "\n",
    "attention_mask11 = np.where(padded11 != 0, 1, 0)\n",
    "attention_mask11.shape\n",
    "input_ids11 = torch.tensor(padded11)  \n",
    "attention_mask11 = torch.tensor(attention_mask11)\n",
    "input_segments11= np.array([_get_segments3(token, max_len11)for token in tokenized11.values])\n",
    "token_type_ids11 = torch.tensor(input_segments11)\n",
    "input_segments11 = torch.tensor(input_segments11)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states11 = model(input_ids11, attention_mask=attention_mask11, token_type_ids=input_segments11)   \n",
    "features11 = last_hidden_states11[0][:,0,:].numpy()\n",
    "features11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223ed1f6-dc4c-4b4c-ac35-d302a2f7aa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#**Classification**\"\"\"\n",
    "\n",
    "features=np.concatenate([features3,features4,features5,features6,features7,features8,features9,features10,features11])\n",
    "\n",
    "\n",
    "\n",
    "features.shape\n",
    "\n",
    "Total = pd.concat([df3,df4,df5,df6,df7,df8,df9,df10,df11], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "labels =Total['Label']\n",
    "labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c103e60f-fa5f-4814-a0dc-a98e978fade5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"hold out \"\"\"\n",
    "\n",
    "# for thunderbird\n",
    "train_features = features[0:6792]\n",
    "train_labels = labels[0:6792]\n",
    "test_features = features[6792:]\n",
    "test_labels = labels[6792:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8f826e-6e99-420a-a5d7-82e9a64fdba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(max_iter=100)\n",
    "from sklearn.datasets import make_classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2833394f-3398-4db7-a55a-155b5caf6e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# **MLP Classifier**\"\"\"\n",
    "\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(50,100,50), (50,100,50), (100,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cd7862-d41a-4716-a700-607099722bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b24d118-0f07-40c9-b4e1-e208b094845d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3)\n",
    "clf.fit(train_features, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fe51da-3123-42a6-8d6b-cd0c3fd84fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best paramete set\n",
    "print('Best parameters found:\\n', clf.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5703d880-0575-48a9-8b83-208c7a880aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All results\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f67bcd6-5693-49a5-9ee0-7e017adc4b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = test_labels , clf.predict(test_features)\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fca5f9-2586-48a9-a6bf-b23f85721834",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print('Results on the test set:')\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(test_labels, y_predLr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c558c0-7052-483d-9224-95873cba5b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(test_labels, y_predLr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933b0d10-e83f-43d0-b9cd-8792cbae7e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#**Logistic Regression**\"\"\"\n",
    "\n",
    "parameters = {'C': np.linspace(0.0001, 100, 20)}\n",
    "grid_search = GridSearchCV(LogisticRegression(), parameters, cv=5)\n",
    "grid_search.fit(train_features, train_labels)\n",
    "print('best parameters: ', grid_search.best_params_)\n",
    "print('best scrores: ', grid_search.best_score_)\n",
    "\n",
    "lr_clf = LogisticRegression(C=10.52)\n",
    "lr_clf.fit(train_features, train_labels)\n",
    "\n",
    "lr_clf.score(test_features, test_labels)\n",
    "\n",
    "y_predLr = lr_clf.predict(test_features)\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "y_predLr\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(test_labels,y_predLr))\n",
    "print(confusion_matrix(test_labels, y_predLr))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(test_labels, y_predLr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a58e15-a603-4c16-8155-f30507ef86f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#**Decision tree**\"\"\"\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth = 500, random_state = 0)\n",
    "\n",
    "clf.fit(train_features, train_labels)\n",
    "\n",
    "y_preddt = clf.predict(test_features)\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "y_preddt\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(test_labels,y_preddt))\n",
    "print(confusion_matrix(test_labels, y_preddt))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(test_labels, y_preddt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6971c18-41ad-43fb-86ff-0de489c1714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#**SVM**\"\"\"\n",
    "\"\"\" **SVC** \"\"\"\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svclassifier = SVC(kernel='linear')\n",
    "svclassifier.fit(train_features, train_labels)\n",
    "\n",
    "y_pred = svclassifier.predict(test_features)\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "y_pred\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(test_labels,y_pred))\n",
    "print(classification_report(test_labels,y_pred))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a32c92-d46b-4522-900a-c564d8be141a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#**Random Forest**\"\"\"\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=20, random_state=0)\n",
    "rf.fit(train_features, train_labels)\n",
    "y_pred1 = rf.predict(test_features)\n",
    "y_pred1\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(test_labels,y_pred1))\n",
    "print(confusion_matrix(test_labels, y_pred1))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(test_labels, y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9521e550-d8fd-47c4-8fd4-8ac3c59fdc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe96a7f-efaf-41c8-95ae-a0a168ca5173",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"**Naive Bayes**\"\"\"\n",
    "# Gaussian\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(train_features, train_labels)\n",
    "\n",
    "y_pred = gnb.predict(test_features)\n",
    "y_pred\n",
    "\n",
    "from sklearn import metrics\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_labels, y_pred))\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(test_labels,y_pred))\n",
    "print(confusion_matrix(test_labels, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446eec91-f326-45ca-9170-f3fbafcaaa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a971913-c7a7-42ad-9f68-c33b567ac67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#**XGBoost**\"\"\"\n",
    "\n",
    "import xgboost as xgb\n",
    "modelxgb=xgb.XGBClassifier(random_state=1,learning_rate=0.01)\n",
    "modelxgb.fit(train_features, train_labels)\n",
    "\n",
    "predxgb = modelxgb.predict(test_features)\n",
    "predxgb\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(test_labels,predxgb))\n",
    "print(confusion_matrix(test_labels, predxgb))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(test_labels, predxgb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f64402e-1d81-4eb3-bcac-47e07ef65d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 假设 train_labels 和 test_labels 是字符串类型的标签\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "# 创建 XGBoost 模型\n",
    "modelxgb = xgb.XGBClassifier(random_state=1, learning_rate=0.01)\n",
    "modelxgb.fit(train_features, train_labels_encoded)  # 使用数值型标签训练模型\n",
    "\n",
    "# 进行预测\n",
    "predxgb = modelxgb.predict(test_features)\n",
    "\n",
    "# 将预测结果转换回原始标签\n",
    "predxgb_labels = label_encoder.inverse_transform(predxgb)\n",
    "\n",
    "# 评估模型性能\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(classification_report(test_labels_encoded, predxgb))  # 使用数值型标签评估\n",
    "print(confusion_matrix(test_labels_encoded, predxgb))  # 使用数值型标签评估\n",
    "print(accuracy_score(test_labels_encoded, predxgb))  # 使用数值型标签评估\n",
    "\n",
    "# 如果需要查看原始标签的评估结果\n",
    "print(classification_report(test_labels, predxgb_labels))  # 使用原始标签评估\n",
    "print(confusion_matrix(test_labels, predxgb_labels))  # 使用原始标签评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d82ea7-fc8d-4127-ac3f-94e8a364780a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#**KNN**\"\"\"\n",
    "\n",
    "#import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Setup arrays to store training and test accuracies\n",
    "neighbors = np.arange(1,9)\n",
    "train_accuracy =np.empty(len(neighbors))\n",
    "test_accuracy = np.empty(len(neighbors))\n",
    "\n",
    "for i,k in enumerate(neighbors):\n",
    "    #Setup a knn classifier with k neighbors\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    \n",
    "    #Fit the model\n",
    "    knn.fit(train_features, train_labels)\n",
    "    \n",
    "    #Compute accuracy on the training set\n",
    "    train_accuracy[i] = knn.score(train_features, train_labels)\n",
    "    \n",
    "    #Compute accuracy on the test set\n",
    "    test_accuracy[i] = knn.score(test_features, test_labels)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2066c073-3338-4351-a014-9a6c06730a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the model\n",
    "knn.fit(train_features,train_labels)\n",
    "\n",
    "knn.score(test_features,test_labels)\n",
    "\n",
    "y_pred = knn.predict(test_features)\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "y_pred\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(test_labels,y_pred))\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_labels,y_pred))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c7cdd5-ab3c-4050-b1a8-1bcbdabcae0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
