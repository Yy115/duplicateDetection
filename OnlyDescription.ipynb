{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1aaac27-9c92-486e-acf8-b58080a615ad",
   "metadata": {},
   "source": [
    "# -*- coding: utf-8 -*-\r\n",
    "\"\"\"Only Description.ipynb\r\n",
    "\r\n",
    "Automatically generated by Colaboratory.\r\n",
    "\r\n",
    "Original file is located at\r\n",
    "    https://colab.research.google.com/drive/19DpflcCSrFj4k79F1sjqQ2PEvgQjWTZ6\r\n",
    "\r\n",
    "##**Installing the transformers library**\r\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "971f0ba1-87d0-445f-b8e2-e02220878576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (4.44.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from transformers) (0.24.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from requests->transformers) (2024.12.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e916f9c5-31f4-48dc-a382-bb8176e042be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"##**Importing the tools**\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch\n",
    "import transformers as ppb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "import re\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d4e3ced-c766-458a-9c5a-729ca64c0ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thunderbird\n",
    "df1=pd.read_csv(r'E:\\PycharmProjects\\duplicate bug report detection\\Duplicate Bug Report\\ThunderBird\\dup_TB.csv',delimiter=';')\n",
    "df2=pd.read_csv(r'E:\\PycharmProjects\\duplicate bug report detection\\Duplicate Bug Report\\ThunderBird\\Nondup_TB.csv',delimiter=';')\n",
    "\n",
    "\n",
    "df1['Label'] = 'duplicate'\n",
    "df2['Label'] = 'non duplicate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "133a51f7-02b6-4628-9324-18df09a400a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"##**Loading the Pre-trained BERT model**\"\"\"\n",
    "\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "640c930f-d2bb-41c2-be78-d3e21efea258",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# **Remove stop words**\"\"\"\n",
    "\n",
    "df1['Title1']= df1['Title1'].str.replace(r'i' 'me' 'my' 'myself' 'we' \n",
    "                                         'our' 'ours' 'ourselves' 'you' 'your' 'yours' 'yourself''yourselves' 'they' 'we' 'him' 'he' 'him' 'his' 'himself' 'she' 'her' 'hers' 'herself' 'it''its' 'itself' 'they' 'them' 'their' 'theirs' 'themselves'  'what' 'which' 'who' 'whom' 'this' 'that'\n",
    "                                         'these' 'those' 'am' 'is' 'are' 'was' 'were' 'be' 'been' 'being' 'have' 'has' 'had' 'having' 'do' 'does' 'did' 'doing' 'a' 'an' 'the' 'and' 'but' 'if' 'or' 'because' 'as' 'until' 'while' 'of' 'at' 'by' 'for' 'with' 'about' 'against' 'between' 'into' 'through' \n",
    "                                         'during' 'before' 'after' 'above' 'below' 'to' 'from' 'up' 'down' 'in' 'out' 'on' 'off' 'over' 'under' 'again' 'further' 'then' 'once'  'here' 'there' 'when' 'where' 'why' 'how' 'all' 'any' 'both' 'each' 'few' 'more' 'most' 'other' 'some' 'such' 'no' 'nor' 'not' 'only' 'own' 'same' 'so' 'than' 'too' 'very' 's' 't' 'can' 'will' 'just' 'don' 'should' 'now'\n",
    "                                         'java' 'com' 'org' ,'')\n",
    "\n",
    "\n",
    "df1['Title2']= df1['Title2'].str.replace(r'i' 'me' 'my' 'myself' 'we' \n",
    "                                         'our' 'ours' 'ourselves' 'you' 'your' 'yours' 'yourself''yourselves' 'they' 'we' 'him' 'he' 'him' 'his' 'himself' 'she' 'her' 'hers' 'herself' 'it''its' 'itself' 'they' 'them' 'their' 'theirs' 'themselves'  'what' 'which' 'who' 'whom' 'this' 'that'\n",
    "                                         'these' 'those' 'am' 'is' 'are' 'was' 'were' 'be' 'been' 'being' 'have' 'has' 'had' 'having' 'do' 'does' 'did' 'doing' 'a' 'an'\n",
    "                                         'the' 'and' 'but' 'if' 'or' 'because' 'as' 'until' 'while' 'of' 'at' 'by' 'for' 'with' 'about' 'against' 'between' 'into' 'through' 'during' 'before' 'after' 'above' 'below' 'to' 'from' 'up' 'down' 'in' 'out' 'on' 'off' 'over' 'under' 'again' 'further' 'then' 'once' 'here' 'there' 'when' 'where' 'why' 'how' 'all' 'any' 'both' 'each' 'few' 'more' 'most' 'other' 'some' 'such' 'no' 'nor' 'not' 'only' 'own' \n",
    "                                         'same' 'so' 'than' 'too' 'very' 's' 't' 'can' 'will' 'just' 'don' 'should' 'now'\n",
    "                                         'java' 'com' 'org' ,'')\n",
    "\n",
    "df2['Title1']= df2['Title1'].str.replace(r'i' 'me' 'my' 'myself' 'we' 'our' 'ours' 'ourselves' 'you' 'your' 'yours' 'yourself''yourselves' 'they' 'we' 'him' 'he' 'him' 'his' 'himself' 'she'\n",
    "                                         'her' 'hers' 'herself' 'it''its' 'itself' 'they' 'them' 'their' 'theirs' 'themselves'  'what' 'which' 'who' 'whom' 'this' 'that' 'these' 'those' 'am' 'is' 'are' 'was' 'were' 'be' 'been' 'being' 'have' 'has' 'had' 'having' 'do' 'does' 'did' 'doing' 'a' 'an'\n",
    "                                         'the' 'and' 'but' 'if' 'or' 'because' 'as' 'until' 'while' 'of' 'at' 'by' 'for' 'with' 'about' 'against' 'between' 'into' 'through' \n",
    "                                         'during' 'before' 'after' 'above' 'below' 'to' 'from' 'up' 'down' 'in' 'out' 'on' 'off' 'over' 'under' 'again' 'further' 'then' 'once' 'here' 'there' 'when' 'where' 'why' 'how' 'all' 'any' 'both' 'each' 'few' 'more' 'most' 'other' 'some' 'such' 'no' 'nor' 'not' 'only' 'own' 'same' 'so' 'than' 'too' 'very' 's' 't' 'can' 'will' 'just' 'don' 'should' 'now'\n",
    "                                         'java' 'com' 'org' ,'')\n",
    "\n",
    "\n",
    "df2['Title2']= df2['Title2'].str.replace(r'i' 'me' 'my' 'myself' 'we' 'our' 'ours' 'ourselves' 'you' 'your' 'yours' 'yourself''yourselves' 'they' 'we' 'him' 'he' 'him' 'his' 'himself' 'she'\n",
    "                                         'her' 'hers' 'herself' 'it''its' 'itself' 'they' 'them' 'their' 'theirs' 'themselves'  'what' 'which' 'who' 'whom' 'this' 'that' 'these' 'those' 'am' 'is' 'are' 'was' 'were' 'be' 'been' 'being' 'have' 'has' 'had' 'having' 'do' 'does' 'did' 'doing' 'a' 'an'\n",
    "                                         'the' 'and' 'but' 'if' 'or' 'because' 'as' 'until' 'while' 'of' 'at' 'by' 'for' 'with' 'about' 'against' 'between' 'into' 'through' \n",
    "                                         'during' 'before' 'after' 'above' 'below' 'to' 'from' 'up' 'down' 'in' 'out' 'on' 'off' 'over' 'under' 'again' 'further' 'then' 'once' 'here' 'there' 'when' 'where' 'why' 'how' 'all' 'any' 'both' 'each' 'few' 'more' 'most' 'other' 'some' 'such' 'no' 'nor' 'not' 'only' 'own' \n",
    "                                         'same' 'so' 'than' 'too' 'very' 's' 't' 'can' 'will' 'just' 'don' 'should' 'now'\n",
    "                                         'java' 'com' 'org' ,'')\n",
    "\n",
    "df1['Description1']= df1['Description1'].str.replace(r'i' 'me' 'my' 'myself' 'we'  'our' 'ours' 'ourselves' 'you' 'your' 'yours' 'yourself''yourselves' 'they' 'we' 'him' 'he' 'him' 'his' 'himself' 'she'\n",
    "                                         'her' 'hers' 'herself' 'it''its' 'itself' 'they' 'them' 'their' 'theirs' 'themselves'  'what' 'which' 'who' 'whom' 'this' 'that' 'these' 'those' 'am' 'is' 'are' 'was' 'were' 'be' 'been' 'being' 'have' 'has' 'had' 'having' 'do' 'does' 'did' 'doing' 'a' 'an' 'the' 'and' 'but' 'if' 'or' 'because' 'as' 'until' 'while' 'of' 'at' 'by' 'for' 'with' 'about' 'against' 'between' 'into' 'through' \n",
    "                                         'during' 'before' 'after' 'above' 'below' 'to' 'from' 'up' 'down' 'in' 'out' 'on' 'off' 'over' 'under' 'again' 'further' 'then' 'once' \n",
    "                                         'here' 'there' 'when' 'where' 'why' 'how' 'all' 'any' 'both' 'each' 'few' 'more' 'most' 'other' 'some' 'such' 'no' 'nor' 'not' 'only' 'own' 'same' 'so' 'than' 'too' 'very' 's' 't' 'can' 'will' 'just' 'don' 'should' 'now'\n",
    "                                         'java' 'com' 'org' ,'')\n",
    "\n",
    "df1['Description2']= df1['Description2'].str.replace(r'i' 'me' 'my' 'myself' 'we' 'our' 'ours' 'ourselves' 'you' 'your' 'yours' 'yourself''yourselves' 'they' 'we' 'him' 'he' 'him' 'his' 'himself' 'she'\n",
    "                                         'her' 'hers' 'herself' 'it''its' 'itself' 'they' 'them' 'their' 'theirs' 'themselves'  'what' 'which' 'who' 'whom' 'this' 'that' 'these' 'those' 'am' 'is' 'are' 'was' 'were' 'be' 'been' 'being' 'have' 'has' 'had' 'having' 'do' 'does' 'did' 'doing' 'a' 'an'\n",
    "                                         'the' 'and' 'but' 'if' 'or' 'because' 'as' 'until' 'while' 'of' 'at' 'by' 'for' 'with' 'about' 'against' 'between' 'into' 'through' \n",
    "                                         'during' 'before' 'after' 'above' 'below' 'to' 'from' 'up' 'down' 'in' 'out' 'on' 'off' 'over' 'under' 'again' 'further' 'then' 'once' 'here' 'there' 'when' 'where' 'why' 'how' 'all' 'any' 'both' 'each' 'few' 'more' 'most' 'other' 'some' 'such' 'no' 'nor' 'not' 'only' 'own' \n",
    "                                         'same' 'so' 'than' 'too' 'very' 's' 't' 'can' 'will' 'just' 'don' 'should' 'now'\n",
    "                                         'java' 'com' 'org' ,'')\n",
    "\n",
    "df2['Description1']= df2['Description1'].str.replace(r'i' 'me' 'my' 'myself' 'we' 'our' 'ours' 'ourselves' 'you' 'your' 'yours' 'yourself''yourselves' 'they' 'we' 'him' 'he' 'him' 'his' 'himself' 'she'\n",
    "                                         'her' 'hers' 'herself' 'it''its' 'itself' 'they' 'them' 'their' 'theirs' 'themselves'  'what' 'which' 'who' 'whom' 'this' 'that' 'these' 'those' 'am' 'is' 'are' 'was' 'were' 'be' 'been' 'being' 'have' 'has' 'had' 'having' 'do' 'does' 'did' 'doing' 'a' 'an'\n",
    "                                         'the' 'and' 'but' 'if' 'or' 'because' 'as' 'until' 'while' 'of' 'at' 'by' 'for' 'with' 'about' 'against' 'between' 'into' 'through' 'during' 'before' 'after' 'above' 'below' 'to' 'from' 'up' 'down' 'in' 'out' 'on' 'off' 'over' 'under' 'again' 'further' 'then' 'once' \n",
    "                                         'here' 'there' 'when' 'where' 'why' 'how' 'all' 'any' 'both' 'each' 'few' 'more' 'most' 'other' 'some' 'such' 'no' 'nor' 'not' 'only' 'own' 'same' 'so' 'than' 'too' 'very' 's' 't' 'can' 'will' 'just' 'don' 'should' 'now'\n",
    "                                         'java' 'com' 'org' ,'')\n",
    "\n",
    "df2['Description2']= df2['Description2'].str.replace(r'i' 'me' 'my' 'myself' 'we' 'our' 'ours' 'ourselves' 'you' 'your' 'yours' 'yourself''yourselves' 'they' 'we' 'him' 'he' 'him' 'his' 'himself' 'she'\n",
    "                                         'her' 'hers' 'herself' 'it''its' 'itself' 'they' 'them' 'their' 'theirs' 'themselves'  'what' 'which' 'who' 'whom' 'this' 'that' 'these' 'those' 'am' 'is' 'are' 'was' 'were' 'be' 'been' 'being' 'have' 'has' 'had' 'having' 'do' 'does' 'did' 'doing' 'a' 'an'\n",
    "                                         'the' 'and' 'but' 'if' 'or' 'because' 'as' 'until' 'while' 'of' 'at' 'by' 'for' 'with' 'about' 'against' 'between' 'into' 'through' 'during' 'before' 'after' 'above' 'below' 'to' 'from' 'up' 'down' 'in' 'out' 'on' 'off' 'over' 'under' 'again' 'further' 'then' 'once' \n",
    "                                         'here' 'there' 'when' 'where' 'why' 'how' 'all' 'any' 'both' 'each' 'few' 'more' 'most' 'other' 'some' 'such' 'no' 'nor' 'not' 'only' 'own'  'same' 'so' 'than' 'too' 'very' 's' 't' 'can' 'will' 'just' 'don' 'should' 'now'\n",
    "                                         'java' 'com' 'org' ,'')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b85de6e4-ba38-415a-852d-ab72538efe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#**Batch ThunderBird**\"\"\"\n",
    "\n",
    "batch_31=df1[:500]\n",
    "batch_32=df2[:500]\n",
    "df3 = pd.concat([batch_31,batch_32], ignore_index=True)\n",
    "batch_41=df1[500:1000]\n",
    "batch_42=df2[500:1000]\n",
    "df4 = pd.concat([batch_41,batch_42], ignore_index=True)\n",
    "batch_51=df1[1000:1500]\n",
    "batch_52=df2[1000:1500]\n",
    "df5 = pd.concat([batch_51,batch_52], ignore_index=True)\n",
    "batch_61=df1[1500:2000]\n",
    "batch_62=df2[1500:2000]\n",
    "df6 = pd.concat([batch_61,batch_62], ignore_index=True)\n",
    "batch_71=df1[2000:2500]\n",
    "batch_72=df2[2000:2500]\n",
    "df7 = pd.concat([batch_71,batch_72], ignore_index=True)\n",
    "batch_81=df1[2500:3000]\n",
    "batch_82=df2[2500:3000]\n",
    "df8 = pd.concat([batch_81,batch_82], ignore_index=True)\n",
    "batch_91=df1[3000:3486]\n",
    "batch_92=df2[3000:3486]\n",
    "df9 = pd.concat([batch_91,batch_92], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cad97f00-bc0c-4e82-b69e-f9c5792ac044",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing\n",
    "batch_101=df1[3486:3900]\n",
    "batch_102=df2[3486:3900]\n",
    "df10 = pd.concat([batch_101,batch_102], ignore_index=True)\n",
    "batch_111=df1[3900:4338]\n",
    "batch_112=df2[3900:4374]\n",
    "df11 = pd.concat([batch_111,batch_112], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa2fe5fd-1227-44e5-8cd5-ddd7cd4bf613",
   "metadata": {},
   "outputs": [],
   "source": [
    "### **_get_segments3**\n",
    "\n",
    "\n",
    "def _get_segments3(tokens, max_seq_length):\n",
    "    # \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    segments = []\n",
    "    first_sep = False\n",
    "    current_segment_id = 0 \n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        #print(token)\n",
    "        if token == 102:\n",
    "            #if first_sep:\n",
    "                #first_sep = False \n",
    "            #else:\n",
    "           current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ee03748-49c4-4def-ae2a-7656700790cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.8002101 , -0.13846451, -0.66763383, ..., -0.6392491 ,\n",
       "         0.30789638,  0.5481622 ],\n",
       "       [-1.0398118 , -0.24063203, -0.50217897, ..., -0.8587797 ,\n",
       "         0.24175009,  0.5938225 ],\n",
       "       [-0.89388585,  0.04131802, -0.32835025, ..., -1.0353571 ,\n",
       "         0.61701906,  0.70739067],\n",
       "       ...,\n",
       "       [-0.85372734, -0.768766  , -0.28845435, ..., -0.3431189 ,\n",
       "        -0.20113057,  0.6494671 ],\n",
       "       [-0.21451108, -0.5956869 , -0.03019422, ..., -0.47669545,\n",
       "         0.11309113,  0.7365488 ],\n",
       "       [-0.8356916 , -0.36011216, -0.23745993, ..., -1.0253073 ,\n",
       "         0.28883314,  0.3716638 ]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"#**df3**\"\"\"\n",
    "\n",
    "pair3= df3['Description1'] +  [\" [SEP] \"] + df3['Description2'] \n",
    "tokenized3 = pair3.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))\n",
    "\n",
    "max_len3 = 0                 # padding all lists to the same size\n",
    "for i in tokenized3.values:\n",
    "    if len(i) > max_len3:\n",
    "        max_len3 = len(i)\n",
    "max_len3 =300\n",
    "padded3 = np.array([i + [0]*(max_len3-len(i)) for i in tokenized3.values])\n",
    "\n",
    "np.array(padded3).shape\n",
    "\n",
    "attention_mask3 = np.where(padded3 != 0, 1, 0)\n",
    "attention_mask3.shape\n",
    "input_ids3 = torch.tensor(padded3)  \n",
    "attention_mask3 = torch.tensor(attention_mask3)\n",
    "input_segments3= np.array([_get_segments3(token, max_len3)for token in tokenized3.values])\n",
    "token_type_ids3 = torch.tensor(input_segments3)\n",
    "input_segments3 = torch.tensor(input_segments3)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states3 = model(input_ids3, attention_mask=attention_mask3, token_type_ids=input_segments3)    # <<< 600 rows only !!!\n",
    "features3 = last_hidden_states3[0][:,0,:].numpy()\n",
    "features3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74045129-14e9-4c99-8163-67610942a33d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.0101286 , -0.37822804, -0.4410481 , ..., -0.46137884,\n",
       "        -0.05894776,  0.6291827 ],\n",
       "       [-0.74701196, -0.20899314,  0.11689647, ..., -0.38710403,\n",
       "         0.37973854,  0.4223293 ],\n",
       "       [-0.62477565, -0.23360267, -0.6645074 , ..., -0.72823703,\n",
       "         0.464076  ,  0.453289  ],\n",
       "       ...,\n",
       "       [-0.45462853, -0.17680955, -0.14185038, ..., -0.672927  ,\n",
       "         0.47661644,  0.54636353],\n",
       "       [-0.5598628 , -0.01987733, -0.00791141, ..., -0.56155914,\n",
       "         0.24467385,  0.5659383 ],\n",
       "       [-0.22766834, -0.10008477,  0.11112246, ..., -0.5797696 ,\n",
       "         0.6146052 ,  0.43150103]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"#**df4**\"\"\"\n",
    "\n",
    "pair4= df4['Description1'] +  [\" [SEP] \"] + df4['Description2'] \n",
    "tokenized4 = pair4.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))\n",
    "max_len4 = 0                 # padding all lists to the same size\n",
    "for i in tokenized4.values:\n",
    "    if len(i) > max_len4:\n",
    "        max_len4 = len(i)\n",
    "max_len4 =300\n",
    "padded4 = np.array([i + [0]*(max_len4-len(i)) for i in tokenized4.values])\n",
    "\n",
    "np.array(padded4).shape\n",
    "\n",
    "attention_mask4 = np.where(padded4 != 0, 1, 0)\n",
    "attention_mask4.shape\n",
    "input_ids4 = torch.tensor(padded4)  \n",
    "attention_mask4 = torch.tensor(attention_mask4)\n",
    "input_segments4= np.array([_get_segments3(token, max_len4)for token in tokenized4.values])\n",
    "token_type_ids4 = torch.tensor(input_segments4)\n",
    "input_segments4 = torch.tensor(input_segments4)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states4 = model(input_ids4, attention_mask=attention_mask4, token_type_ids=input_segments4)   \n",
    "features4 = last_hidden_states4[0][:,0,:].numpy()\n",
    "features4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8e8a8bf-0e79-424d-a77f-5b8a3e525374",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#**df5**\"\"\"\n",
    "\n",
    "pair5= df5['Description1'] +  [\" [SEP] \"] + df5['Description2'] \n",
    "tokenized5 = pair5.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2d21c4a-1a7f-4d6e-8d8d-ab210083df30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 300)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"##**Padding**\"\"\"\n",
    "\n",
    "max_len5 = 0                 # padding all lists to the same size\n",
    "for i in tokenized5.values:\n",
    "    if len(i) > max_len5:\n",
    "        max_len5 = len(i)\n",
    "\n",
    "max_len5 =300\n",
    "padded5 = np.array([i + [0]*(max_len5-len(i)) for i in tokenized5.values])\n",
    "\n",
    "np.array(padded5).shape        # Dimensions of the padded variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75da72e0-4713-486b-9920-ce144fdf817b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"##**Masking**\"\"\"\n",
    "\n",
    "attention_mask5 = np.where(padded5 != 0, 1, 0)\n",
    "attention_mask5.shape\n",
    "input_ids5 = torch.tensor(padded5)  \n",
    "attention_mask5 = torch.tensor(attention_mask5)\n",
    "\n",
    "\"\"\"##**Running the `model()` function through BERT**\"\"\"\n",
    "\n",
    "input_segments5= np.array([_get_segments3(token, max_len5)for token in tokenized5.values])\n",
    "token_type_ids5 = torch.tensor(input_segments5)\n",
    "input_segments5 = torch.tensor(input_segments5)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states5 = model(input_ids5, attention_mask=attention_mask5, token_type_ids=input_segments5)    # <<< 600 rows only !!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f444df31-f4f6-4be5-be04-e6577b1ccb8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.72306997, -0.11034629, -0.4326458 , ..., -0.8139001 ,\n",
       "         0.5231651 ,  0.46807176],\n",
       "       [-1.0178485 , -0.23369282, -0.75839365, ..., -0.9670361 ,\n",
       "         0.3748719 ,  0.31157652],\n",
       "       [-0.73865384, -0.05767574, -0.2978505 , ..., -0.70388937,\n",
       "         0.32853636,  0.70095587],\n",
       "       ...,\n",
       "       [-0.88516074, -0.3034544 , -0.14182365, ..., -0.78002286,\n",
       "         0.49511454,  0.59836024],\n",
       "       [-0.31329042,  0.28976032, -0.31038925, ..., -0.709839  ,\n",
       "         0.6801357 ,  0.30287015],\n",
       "       [-0.6600901 ,  0.2824442 , -0.07917926, ..., -0.77998036,\n",
       "         0.34385332,  0.6198041 ]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"##**Slicing the part of the output of BERT : [cls]**\"\"\"\n",
    "\n",
    "features5 = last_hidden_states5[0][:,0,:].numpy()\n",
    "features5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51412359-fb18-4cc3-9dd4-4aa2c8f82862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.1090034 , -0.03982682, -0.6715141 , ..., -0.80681497,\n",
       "         0.01517439,  0.3499201 ],\n",
       "       [-1.2283841 , -0.39848366, -0.40330702, ..., -0.80732286,\n",
       "         0.19202395,  0.3646002 ],\n",
       "       [-1.0270618 , -0.04674022, -0.32805264, ..., -0.84174454,\n",
       "         0.07392713,  0.3245452 ],\n",
       "       ...,\n",
       "       [-0.21842498, -0.4814207 ,  0.13334791, ..., -0.5921028 ,\n",
       "         0.43032563,  0.75576717],\n",
       "       [-0.9030674 ,  0.01523039,  0.12806195, ..., -0.74598557,\n",
       "         0.57876277,  0.71984667],\n",
       "       [-0.88152856, -0.1368047 , -0.62479585, ..., -0.8435278 ,\n",
       "         0.49850908,  0.56405663]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"#**df6**\"\"\"\n",
    "\n",
    "pair6= df6['Description1'] +  [\" [SEP] \"] + df6['Description2'] \n",
    "tokenized6 = pair6.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))\n",
    "\n",
    "max_len6 = 0                 # padding all lists to the same size\n",
    "for i in tokenized6.values:\n",
    "    if len(i) > max_len6:\n",
    "        max_len6 = len(i)\n",
    "\n",
    "max_len6=300\n",
    "padded6 = np.array([i + [0]*(max_len6-len(i)) for i in tokenized6.values])\n",
    "\n",
    "np.array(padded6).shape        # Dimensions of the padded variable        \n",
    "\n",
    "attention_mask6 = np.where(padded6 != 0, 1, 0)\n",
    "attention_mask6.shape\n",
    "input_ids6 = torch.tensor(padded6)  \n",
    "attention_mask6 = torch.tensor(attention_mask6)\n",
    "input_segments6= np.array([_get_segments3(token, max_len6)for token in tokenized6.values])\n",
    "token_type_ids6 = torch.tensor(input_segments6)\n",
    "input_segments6 = torch.tensor(input_segments6)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states6 = model(input_ids6, attention_mask=attention_mask6, token_type_ids=input_segments6)   \n",
    "features6 = last_hidden_states6[0][:,0,:].numpy()\n",
    "features6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "325e8acb-1f66-47e7-800f-a13f1ea05f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.78490067, -0.3908618 , -0.01803618, ..., -0.39361522,\n",
       "         0.25720027,  0.48736867],\n",
       "       [-1.137159  , -0.22095528, -0.2966019 , ..., -0.9068767 ,\n",
       "         0.32979396,  0.6857854 ],\n",
       "       [-0.78578395,  0.18438455, -0.7090969 , ..., -0.81829697,\n",
       "         0.48674095,  0.32515293],\n",
       "       ...,\n",
       "       [-0.83848375,  0.03468208, -0.6112732 , ..., -0.82992435,\n",
       "         0.4256623 ,  0.59070355],\n",
       "       [-0.546866  , -1.3112406 ,  0.39031634, ..., -0.1988132 ,\n",
       "        -0.15689509,  0.5340398 ],\n",
       "       [-0.666705  , -0.9124074 ,  0.16915879, ..., -0.28950933,\n",
       "        -0.40293646,  1.1284608 ]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"#**df7**\"\"\"\n",
    "pair7= df7['Description1'] +  [\" [SEP] \"] + df7['Description2'] \n",
    "tokenized7 = pair7.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))\n",
    "\n",
    "max_len7 = 0                 # padding all lists to the same size\n",
    "for i in tokenized7.values:\n",
    "    if len(i) > max_len7:\n",
    "        max_len7 = len(i)\n",
    "\n",
    "max_len7=300\n",
    "padded7 = np.array([i + [0]*(max_len7-len(i)) for i in tokenized7.values])\n",
    "\n",
    "np.array(padded7).shape        # Dimensions of the padded variable    \n",
    "\n",
    "attention_mask7 = np.where(padded7 != 0, 1, 0)\n",
    "attention_mask7.shape\n",
    "input_ids7 = torch.tensor(padded7)  \n",
    "attention_mask7 = torch.tensor(attention_mask7)\n",
    "input_segments7= np.array([_get_segments3(token, max_len7)for token in tokenized7.values])\n",
    "token_type_ids7 = torch.tensor(input_segments7)\n",
    "input_segments7 = torch.tensor(input_segments7)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states7 = model(input_ids7, attention_mask=attention_mask7, token_type_ids=input_segments7)  \n",
    "features7 = last_hidden_states7[0][:,0,:].numpy()\n",
    "features7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44893314-5de8-4fcc-a6cc-cbee5732da81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#**df8**\"\"\"\n",
    "pair8= df8['Description1'] +  [\" [SEP] \"] + df8['Description2'] \n",
    "tokenized8 = pair8.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))\n",
    "\n",
    "max_len8 = 0                 # padding all lists to the same size\n",
    "for i in tokenized8.values:\n",
    "    if len(i) > max_len8:\n",
    "        max_len8 = len(i)\n",
    "max_len8=300\n",
    "padded8 = np.array([i + [0]*(max_len8-len(i)) for i in tokenized8.values])\n",
    "\n",
    "np.array(padded8).shape        # Dimensions of the padded variable  \n",
    "\n",
    "\n",
    "attention_mask8 = np.where(padded8 != 0, 1, 0)\n",
    "attention_mask8.shape\n",
    "input_ids8 = torch.tensor(padded8)  \n",
    "attention_mask8 = torch.tensor(attention_mask8)\n",
    "input_segments8= np.array([_get_segments3(token, max_len8)for token in tokenized8.values])\n",
    "token_type_ids8 = torch.tensor(input_segments8)\n",
    "input_segments8 = torch.tensor(input_segments8)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states8 = model(input_ids8, attention_mask=attention_mask8, token_type_ids=input_segments8)   \n",
    "features8 = last_hidden_states8[0][:,0,:].numpy()\n",
    "features8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22de16ad-24d9-412f-b68a-4ad386257dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#**df9**\"\"\"\n",
    "pair9= df9['Description1'] +  [\" [SEP] \"] + df9['Description2'] \n",
    "tokenized9 = pair9.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))\n",
    "\n",
    "max_len9 = 0                 # padding all lists to the same size\n",
    "for i in tokenized9.values:\n",
    "    if len(i) > max_len9:\n",
    "        max_len9 = len(i)\n",
    "max_len9=300\n",
    "padded9 = np.array([i + [0]*(max_len9-len(i)) for i in tokenized9.values])\n",
    "\n",
    "np.array(padded9).shape        # Dimensions of the padded variable    \n",
    "\n",
    "attention_mask9 = np.where(padded9 != 0, 1, 0)\n",
    "attention_mask9.shape\n",
    "input_ids9 = torch.tensor(padded9)  \n",
    "attention_mask9 = torch.tensor(attention_mask9)\n",
    "input_segments9= np.array([_get_segments3(token, max_len9)for token in tokenized9.values])\n",
    "token_type_ids9 = torch.tensor(input_segments9)\n",
    "input_segments9 = torch.tensor(input_segments9)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states9 = model(input_ids9, attention_mask=attention_mask9, token_type_ids=input_segments9)    \n",
    "features9 = last_hidden_states9[0][:,0,:].numpy()\n",
    "features9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbfed68-5859-425e-9313-49b7fbb6e958",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#**df10**\"\"\"\n",
    "\n",
    "pair10= df10['Description1'] +  [\" [SEP] \"] + df10['Description2'] \n",
    "tokenized10 = pair10.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))\n",
    "max_len10 = 0                 # padding all lists to the same size\n",
    "for i in tokenized10.values:\n",
    "    if len(i) > max_len10:\n",
    "        max_len10 = len(i)\n",
    "max_len10=300\n",
    "padded10 = np.array([i + [0]*(max_len10-len(i)) for i in tokenized10.values])\n",
    "\n",
    "np.array(padded10).shape        # Dimensions of the padded variable\n",
    "\n",
    "attention_mask10 = np.where(padded10 != 0, 1, 0)\n",
    "attention_mask10.shape\n",
    "input_ids10 = torch.tensor(padded10)  \n",
    "attention_mask10 = torch.tensor(attention_mask10)\n",
    "input_segments10= np.array([_get_segments3(token, max_len10)for token in tokenized10.values])\n",
    "token_type_ids10 = torch.tensor(input_segments10)\n",
    "input_segments10 = torch.tensor(input_segments10)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states10 = model(input_ids10, attention_mask=attention_mask10, token_type_ids=input_segments10) \n",
    "features10 = last_hidden_states10[0][:,0,:].numpy()\n",
    "features10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3d6a54-7883-4b1d-8224-ab25069c2d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#**df11**\"\"\"\n",
    "\n",
    "pair11= df11['Description1'] +  [\" [SEP] \"] + df11['Description2'] \n",
    "tokenized11 = pair11.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))\n",
    "\n",
    "max_len11 = 0                 # padding all lists to the same size\n",
    "for i in tokenized11.values:\n",
    "    if len(i) > max_len11:\n",
    "        max_len11 = len(i)\n",
    "max_len11=300\n",
    "padded11 = np.array([i + [0]*(max_len11-len(i)) for i in tokenized11.values])\n",
    "\n",
    "np.array(padded11).shape        # Dimensions of the padded variable   \n",
    "\n",
    "attention_mask11 = np.where(padded11 != 0, 1, 0)\n",
    "attention_mask11.shape\n",
    "input_ids11 = torch.tensor(padded11)  \n",
    "attention_mask11 = torch.tensor(attention_mask11)\n",
    "input_segments11= np.array([_get_segments3(token, max_len11)for token in tokenized11.values])\n",
    "token_type_ids11 = torch.tensor(input_segments11)\n",
    "input_segments11 = torch.tensor(input_segments11)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states11 = model(input_ids11, attention_mask=attention_mask11, token_type_ids=input_segments11)   \n",
    "features11 = last_hidden_states11[0][:,0,:].numpy()\n",
    "features11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb497ff-6bb8-4477-af92-7401db4a2b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#**Classification**\"\"\"\n",
    "\n",
    "features=np.concatenate([features3,features4,features5,features6,features7,features8,features9,features10,features11])\n",
    "\n",
    "features.shape\n",
    "\n",
    "Total = pd.concat([df3,df4,df5,df6,df7,df8,df9,df10,df11], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "labels =Total['Label']\n",
    "labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e4f3a2-f85c-48bd-a2bf-1316898b27c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"hold out \"\"\"\n",
    "\n",
    "# for thunderbird\n",
    "train_features = features[0:6792]\n",
    "train_labels = labels[0:6792]\n",
    "test_features = features[6792:]\n",
    "test_labels = labels[6792:]\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(max_iter=100)\n",
    "from sklearn.datasets import make_classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243cac13-6cea-4d89-a7cf-fab36c35ae7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# **MLP Classifier**\"\"\"\n",
    "\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(50,100,50), (50,100,50), (100,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3)\n",
    "clf.fit(train_features, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b84244-8aa6-47e9-9fe5-ee8e88fedae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best paramete set\n",
    "print('Best parameters found:\\n', clf.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f967a44f-a7a6-4403-9139-e659fc83444a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All results\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec486170-aed7-4348-8521-4a8fc707e767",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = test_labels , clf.predict(test_features)\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd78329-cce6-489a-831c-c0f2fa4edcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print('Results on the test set:')\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e9f9b8-0d50-4786-8b15-25fc463a34b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813b62a5-a525-4abc-a1bb-a91b4c6ab873",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
