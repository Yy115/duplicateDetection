{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ff2c1bf-b00f-4b80-8d4e-4ebc5af3ba7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (4.44.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from transformers) (0.24.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from requests->transformers) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02d81180-c5e7-4aa4-8218-8da031fa20aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"##**Importing the tools**\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch\n",
    "import transformers as ppb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "import re\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f799f1c8-6368-418b-a2df-35e6e6723f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thunderbird\n",
    "df1=pd.read_csv(r'E:\\PycharmProjects\\duplicate bug report detection\\Duplicate Bug Report\\ThunderBird\\dup_TB.csv',delimiter=';')\n",
    "df2=pd.read_csv(r'E:\\PycharmProjects\\duplicate bug report detection\\Duplicate Bug Report\\ThunderBird\\Nondup_TB.csv',delimiter=';')\n",
    "\n",
    "\n",
    "df1['Label'] = 'duplicate'\n",
    "df2['Label'] = 'non duplicate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8171083f-6ffb-41b5-b936-3ea865ce9ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"##**Loading the Pre-trained BERT model**\"\"\"\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a678b7e-cec7-40db-ab4f-ab3171dc4c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"##**Loading the Pre-trained BERT model**\"\"\"\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)\n",
    "\n",
    "\"\"\"# **Remove stop words**\"\"\"\n",
    "\n",
    "df1['Title1']= df1['Title1'].str.replace(r'i' 'me' 'my' 'myself' 'we' \n",
    "                                         'our' 'ours' 'ourselves' 'you' 'your' 'yours' 'yourself''yourselves' 'they' 'we' 'him' 'he' 'him' 'his' 'himself' 'she' 'her' 'hers' 'herself' 'it''its' 'itself' 'they' 'them' 'their' 'theirs' 'themselves'  'what' 'which' 'who' 'whom' 'this' 'that'\n",
    "                                         'these' 'those' 'am' 'is' 'are' 'was' 'were' 'be' 'been' 'being' 'have' 'has' 'had' 'having' 'do' 'does' 'did' 'doing' 'a' 'an' 'the' 'and' 'but' 'if' 'or' 'because' 'as' 'until' 'while' 'of' 'at' 'by' 'for' 'with' 'about' 'against' 'between' 'into' 'through' \n",
    "                                         'during' 'before' 'after' 'above' 'below' 'to' 'from' 'up' 'down' 'in' 'out' 'on' 'off' 'over' 'under' 'again' 'further' 'then' 'once'  'here' 'there' 'when' 'where' 'why' 'how' 'all' 'any' 'both' 'each' 'few' 'more' 'most' 'other' 'some' 'such' 'no' 'nor' 'not' 'only' 'own' 'same' 'so' 'than' 'too' 'very' 's' 't' 'can' 'will' 'just' 'don' 'should' 'now'\n",
    "                                         'java' 'com' 'org' ,'')\n",
    "\n",
    "\n",
    "df1['Title2']= df1['Title2'].str.replace(r'i' 'me' 'my' 'myself' 'we' \n",
    "                                         'our' 'ours' 'ourselves' 'you' 'your' 'yours' 'yourself''yourselves' 'they' 'we' 'him' 'he' 'him' 'his' 'himself' 'she' 'her' 'hers' 'herself' 'it''its' 'itself' 'they' 'them' 'their' 'theirs' 'themselves'  'what' 'which' 'who' 'whom' 'this' 'that'\n",
    "                                         'these' 'those' 'am' 'is' 'are' 'was' 'were' 'be' 'been' 'being' 'have' 'has' 'had' 'having' 'do' 'does' 'did' 'doing' 'a' 'an'\n",
    "                                         'the' 'and' 'but' 'if' 'or' 'because' 'as' 'until' 'while' 'of' 'at' 'by' 'for' 'with' 'about' 'against' 'between' 'into' 'through' 'during' 'before' 'after' 'above' 'below' 'to' 'from' 'up' 'down' 'in' 'out' 'on' 'off' 'over' 'under' 'again' 'further' 'then' 'once' 'here' 'there' 'when' 'where' 'why' 'how' 'all' 'any' 'both' 'each' 'few' 'more' 'most' 'other' 'some' 'such' 'no' 'nor' 'not' 'only' 'own' \n",
    "                                         'same' 'so' 'than' 'too' 'very' 's' 't' 'can' 'will' 'just' 'don' 'should' 'now'\n",
    "                                         'java' 'com' 'org' ,'')\n",
    "\n",
    "df2['Title1']= df2['Title1'].str.replace(r'i' 'me' 'my' 'myself' 'we' 'our' 'ours' 'ourselves' 'you' 'your' 'yours' 'yourself''yourselves' 'they' 'we' 'him' 'he' 'him' 'his' 'himself' 'she'\n",
    "                                         'her' 'hers' 'herself' 'it''its' 'itself' 'they' 'them' 'their' 'theirs' 'themselves'  'what' 'which' 'who' 'whom' 'this' 'that' 'these' 'those' 'am' 'is' 'are' 'was' 'were' 'be' 'been' 'being' 'have' 'has' 'had' 'having' 'do' 'does' 'did' 'doing' 'a' 'an'\n",
    "                                         'the' 'and' 'but' 'if' 'or' 'because' 'as' 'until' 'while' 'of' 'at' 'by' 'for' 'with' 'about' 'against' 'between' 'into' 'through' \n",
    "                                         'during' 'before' 'after' 'above' 'below' 'to' 'from' 'up' 'down' 'in' 'out' 'on' 'off' 'over' 'under' 'again' 'further' 'then' 'once' 'here' 'there' 'when' 'where' 'why' 'how' 'all' 'any' 'both' 'each' 'few' 'more' 'most' 'other' 'some' 'such' 'no' 'nor' 'not' 'only' 'own' 'same' 'so' 'than' 'too' 'very' 's' 't' 'can' 'will' 'just' 'don' 'should' 'now'\n",
    "                                         'java' 'com' 'org' ,'')\n",
    "\n",
    "\n",
    "df2['Title2']= df2['Title2'].str.replace(r'i' 'me' 'my' 'myself' 'we' 'our' 'ours' 'ourselves' 'you' 'your' 'yours' 'yourself''yourselves' 'they' 'we' 'him' 'he' 'him' 'his' 'himself' 'she'\n",
    "                                         'her' 'hers' 'herself' 'it''its' 'itself' 'they' 'them' 'their' 'theirs' 'themselves'  'what' 'which' 'who' 'whom' 'this' 'that' 'these' 'those' 'am' 'is' 'are' 'was' 'were' 'be' 'been' 'being' 'have' 'has' 'had' 'having' 'do' 'does' 'did' 'doing' 'a' 'an'\n",
    "                                         'the' 'and' 'but' 'if' 'or' 'because' 'as' 'until' 'while' 'of' 'at' 'by' 'for' 'with' 'about' 'against' 'between' 'into' 'through' \n",
    "                                         'during' 'before' 'after' 'above' 'below' 'to' 'from' 'up' 'down' 'in' 'out' 'on' 'off' 'over' 'under' 'again' 'further' 'then' 'once' 'here' 'there' 'when' 'where' 'why' 'how' 'all' 'any' 'both' 'each' 'few' 'more' 'most' 'other' 'some' 'such' 'no' 'nor' 'not' 'only' 'own' \n",
    "                                         'same' 'so' 'than' 'too' 'very' 's' 't' 'can' 'will' 'just' 'don' 'should' 'now'\n",
    "                                         'java' 'com' 'org' ,'')\n",
    "\n",
    "df1['Description1']= df1['Description1'].str.replace(r'i' 'me' 'my' 'myself' 'we'  'our' 'ours' 'ourselves' 'you' 'your' 'yours' 'yourself''yourselves' 'they' 'we' 'him' 'he' 'him' 'his' 'himself' 'she'\n",
    "                                         'her' 'hers' 'herself' 'it''its' 'itself' 'they' 'them' 'their' 'theirs' 'themselves'  'what' 'which' 'who' 'whom' 'this' 'that' 'these' 'those' 'am' 'is' 'are' 'was' 'were' 'be' 'been' 'being' 'have' 'has' 'had' 'having' 'do' 'does' 'did' 'doing' 'a' 'an' 'the' 'and' 'but' 'if' 'or' 'because' 'as' 'until' 'while' 'of' 'at' 'by' 'for' 'with' 'about' 'against' 'between' 'into' 'through' \n",
    "                                         'during' 'before' 'after' 'above' 'below' 'to' 'from' 'up' 'down' 'in' 'out' 'on' 'off' 'over' 'under' 'again' 'further' 'then' 'once' \n",
    "                                         'here' 'there' 'when' 'where' 'why' 'how' 'all' 'any' 'both' 'each' 'few' 'more' 'most' 'other' 'some' 'such' 'no' 'nor' 'not' 'only' 'own' 'same' 'so' 'than' 'too' 'very' 's' 't' 'can' 'will' 'just' 'don' 'should' 'now'\n",
    "                                         'java' 'com' 'org' ,'')\n",
    "\n",
    "df1['Description2']= df1['Description2'].str.replace(r'i' 'me' 'my' 'myself' 'we' 'our' 'ours' 'ourselves' 'you' 'your' 'yours' 'yourself''yourselves' 'they' 'we' 'him' 'he' 'him' 'his' 'himself' 'she'\n",
    "                                         'her' 'hers' 'herself' 'it''its' 'itself' 'they' 'them' 'their' 'theirs' 'themselves'  'what' 'which' 'who' 'whom' 'this' 'that' 'these' 'those' 'am' 'is' 'are' 'was' 'were' 'be' 'been' 'being' 'have' 'has' 'had' 'having' 'do' 'does' 'did' 'doing' 'a' 'an'\n",
    "                                         'the' 'and' 'but' 'if' 'or' 'because' 'as' 'until' 'while' 'of' 'at' 'by' 'for' 'with' 'about' 'against' 'between' 'into' 'through' \n",
    "                                         'during' 'before' 'after' 'above' 'below' 'to' 'from' 'up' 'down' 'in' 'out' 'on' 'off' 'over' 'under' 'again' 'further' 'then' 'once' 'here' 'there' 'when' 'where' 'why' 'how' 'all' 'any' 'both' 'each' 'few' 'more' 'most' 'other' 'some' 'such' 'no' 'nor' 'not' 'only' 'own' \n",
    "                                         'same' 'so' 'than' 'too' 'very' 's' 't' 'can' 'will' 'just' 'don' 'should' 'now'\n",
    "                                         'java' 'com' 'org' ,'')\n",
    "\n",
    "df2['Description1']= df2['Description1'].str.replace(r'i' 'me' 'my' 'myself' 'we' 'our' 'ours' 'ourselves' 'you' 'your' 'yours' 'yourself''yourselves' 'they' 'we' 'him' 'he' 'him' 'his' 'himself' 'she'\n",
    "                                         'her' 'hers' 'herself' 'it''its' 'itself' 'they' 'them' 'their' 'theirs' 'themselves'  'what' 'which' 'who' 'whom' 'this' 'that' 'these' 'those' 'am' 'is' 'are' 'was' 'were' 'be' 'been' 'being' 'have' 'has' 'had' 'having' 'do' 'does' 'did' 'doing' 'a' 'an'\n",
    "                                         'the' 'and' 'but' 'if' 'or' 'because' 'as' 'until' 'while' 'of' 'at' 'by' 'for' 'with' 'about' 'against' 'between' 'into' 'through' 'during' 'before' 'after' 'above' 'below' 'to' 'from' 'up' 'down' 'in' 'out' 'on' 'off' 'over' 'under' 'again' 'further' 'then' 'once' \n",
    "                                         'here' 'there' 'when' 'where' 'why' 'how' 'all' 'any' 'both' 'each' 'few' 'more' 'most' 'other' 'some' 'such' 'no' 'nor' 'not' 'only' 'own' 'same' 'so' 'than' 'too' 'very' 's' 't' 'can' 'will' 'just' 'don' 'should' 'now'\n",
    "                                         'java' 'com' 'org' ,'')\n",
    "\n",
    "df2['Description2']= df2['Description2'].str.replace(r'i' 'me' 'my' 'myself' 'we' 'our' 'ours' 'ourselves' 'you' 'your' 'yours' 'yourself''yourselves' 'they' 'we' 'him' 'he' 'him' 'his' 'himself' 'she'\n",
    "                                         'her' 'hers' 'herself' 'it''its' 'itself' 'they' 'them' 'their' 'theirs' 'themselves'  'what' 'which' 'who' 'whom' 'this' 'that' 'these' 'those' 'am' 'is' 'are' 'was' 'were' 'be' 'been' 'being' 'have' 'has' 'had' 'having' 'do' 'does' 'did' 'doing' 'a' 'an'\n",
    "                                         'the' 'and' 'but' 'if' 'or' 'because' 'as' 'until' 'while' 'of' 'at' 'by' 'for' 'with' 'about' 'against' 'between' 'into' 'through' 'during' 'before' 'after' 'above' 'below' 'to' 'from' 'up' 'down' 'in' 'out' 'on' 'off' 'over' 'under' 'again' 'further' 'then' 'once' \n",
    "                                         'here' 'there' 'when' 'where' 'why' 'how' 'all' 'any' 'both' 'each' 'few' 'more' 'most' 'other' 'some' 'such' 'no' 'nor' 'not' 'only' 'own'  'same' 'so' 'than' 'too' 'very' 's' 't' 'can' 'will' 'just' 'don' 'should' 'now'\n",
    "                                         'java' 'com' 'org' ,'')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69ceb72-8537-478b-9edf-ae75df7ce9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#**Batch ThunderBird**\"\"\"\n",
    "\n",
    "batch_31=df1[:500]\n",
    "batch_32=df2[:500]\n",
    "df3 = pd.concat([batch_31,batch_32], ignore_index=True)\n",
    "batch_41=df1[500:1000]\n",
    "batch_42=df2[500:1000]\n",
    "df4 = pd.concat([batch_41,batch_42], ignore_index=True)\n",
    "batch_51=df1[1000:1500]\n",
    "batch_52=df2[1000:1500]\n",
    "df5 = pd.concat([batch_51,batch_52], ignore_index=True)\n",
    "batch_61=df1[1500:2000]\n",
    "batch_62=df2[1500:2000]\n",
    "df6 = pd.concat([batch_61,batch_62], ignore_index=True)\n",
    "batch_71=df1[2000:2500]\n",
    "batch_72=df2[2000:2500]\n",
    "df7 = pd.concat([batch_71,batch_72], ignore_index=True)\n",
    "batch_81=df1[2500:3000]\n",
    "batch_82=df2[2500:3000]\n",
    "df8 = pd.concat([batch_81,batch_82], ignore_index=True)\n",
    "batch_91=df1[3000:3486]\n",
    "batch_92=df2[3000:3486]\n",
    "df9 = pd.concat([batch_91,batch_92], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dca6ff-3dd7-4eb1-948d-61c74479297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing\n",
    "batch_101=df1[3486:3900]\n",
    "batch_102=df2[3486:3900]\n",
    "df10 = pd.concat([batch_101,batch_102], ignore_index=True)\n",
    "batch_111=df1[3900:4338]\n",
    "batch_112=df2[3900:4374]\n",
    "df11 = pd.concat([batch_111,batch_112], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb9173b-f210-4a34-be32-1bdbaa4b7b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" **_get_segments3** \"\"\"\n",
    "\n",
    "def _get_segments3(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    segments = []\n",
    "    first_sep = False\n",
    "    current_segment_id = 0 \n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        #print(token)\n",
    "        if token == 102:\n",
    "            #if first_sep:\n",
    "                #first_sep = False \n",
    "            #else:\n",
    "           current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14d130d-f4a4-4622-90e0-519b1c255ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#**df3**\"\"\"\n",
    "\n",
    "pair3= df3['Title1'] + df3['Description1']+ [\" [SEP] \"] + df3['Title2'] + df3['Description2']\n",
    "tokenized3 = pair3.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))\n",
    "\n",
    "max_len3 = 0                 # padding all lists to the same size\n",
    "for i in tokenized3.values:\n",
    "    if len(i) > max_len3:\n",
    "        max_len3 = len(i)\n",
    "max_len3 =300\n",
    "padded3 = np.array([i + [0]*(max_len3-len(i)) for i in tokenized3.values])\n",
    "\n",
    "np.array(padded3).shape\n",
    "\n",
    "attention_mask3 = np.where(padded3 != 0, 1, 0)\n",
    "attention_mask3.shape\n",
    "input_ids3 = torch.tensor(padded3)  \n",
    "attention_mask3 = torch.tensor(attention_mask3)\n",
    "input_segments3= np.array([_get_segments3(token, max_len3)for token in tokenized3.values])\n",
    "token_type_ids3 = torch.tensor(input_segments3)\n",
    "input_segments3 = torch.tensor(input_segments3)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states3 = model(input_ids3, attention_mask=attention_mask3, token_type_ids=input_segments3)    # <<< 600 rows only !!!\n",
    "features3 = last_hidden_states3[0][:,0,:].numpy()\n",
    "features3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0184f7f1-9929-4d09-b546-bdb101cbf626",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#**df4**\"\"\"\n",
    "\n",
    "pair4=df4['Title1'] + df4['Description1']+ [\" [SEP] \"] + df4['Title2']  + df4['Description2']\n",
    "tokenized4 = pair4.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))\n",
    "max_len4 = 0                 # padding all lists to the same size\n",
    "for i in tokenized4.values:\n",
    "    if len(i) > max_len4:\n",
    "        max_len4 = len(i)\n",
    "max_len4 =300\n",
    "padded4 = np.array([i + [0]*(max_len4-len(i)) for i in tokenized4.values])\n",
    "\n",
    "np.array(padded4).shape\n",
    "\n",
    "attention_mask4 = np.where(padded4 != 0, 1, 0)\n",
    "attention_mask4.shape\n",
    "input_ids4 = torch.tensor(padded4)  \n",
    "attention_mask4 = torch.tensor(attention_mask4)\n",
    "input_segments4= np.array([_get_segments3(token, max_len4)for token in tokenized4.values])\n",
    "token_type_ids4 = torch.tensor(input_segments4)\n",
    "input_segments4 = torch.tensor(input_segments4)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states4 = model(input_ids4, attention_mask=attention_mask4, token_type_ids=input_segments4)   \n",
    "features4 = last_hidden_states4[0][:,0,:].numpy()\n",
    "features4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d510f21-180f-4942-84db-e85bf724f278",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#**df5**\"\"\"\n",
    "\n",
    "pair5=df5['Title1'] + df5['Description1']+ [\" [SEP] \"] + df5['Title2'] + df5['Description2']\n",
    "tokenized5 = pair5.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c47e94-98f9-4fd6-92ab-a40fa516ead4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"##**Padding**\"\"\"\n",
    "\n",
    "max_len5 = 0                 # padding all lists to the same size\n",
    "for i in tokenized5.values:\n",
    "    if len(i) > max_len5:\n",
    "        max_len5 = len(i)\n",
    "\n",
    "max_len5 =300\n",
    "padded5 = np.array([i + [0]*(max_len5-len(i)) for i in tokenized5.values])\n",
    "\n",
    "np.array(padded5).shape        # Dimensions of the padded variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ad5e4a-3688-4162-9499-f42481c842ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"##**Masking**\"\"\"\n",
    "\n",
    "attention_mask5 = np.where(padded5 != 0, 1, 0)\n",
    "attention_mask5.shape\n",
    "input_ids5 = torch.tensor(padded5)  \n",
    "attention_mask5 = torch.tensor(attention_mask5)\n",
    "\n",
    "\"\"\"##**Running the `model()` function through BERT**\"\"\"\n",
    "\n",
    "input_segments5= np.array([_get_segments3(token, max_len5)for token in tokenized5.values])\n",
    "token_type_ids5 = torch.tensor(input_segments5)\n",
    "input_segments5 = torch.tensor(input_segments5)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states5 = model(input_ids5, attention_mask=attention_mask5, token_type_ids=input_segments5)    # <<< 600 rows only !!!\n",
    "\n",
    "\"\"\"##**Slicing the part of the output of BERT : [cls]**\"\"\"\n",
    "\n",
    "features5 = last_hidden_states5[0][:,0,:].numpy()\n",
    "features5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2efa327-07af-4113-b28a-c45277b834e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#**df6**\"\"\"\n",
    "\n",
    "pair6=df6['Title1'] + df6['Description1']+ [\" [SEP] \"] + df6['Title2'] + df6['Description2']\n",
    "tokenized6 = pair6.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))\n",
    "\n",
    "max_len6 = 0                 # padding all lists to the same size\n",
    "for i in tokenized6.values:\n",
    "    if len(i) > max_len6:\n",
    "        max_len6 = len(i)\n",
    "\n",
    "max_len6=300\n",
    "padded6 = np.array([i + [0]*(max_len6-len(i)) for i in tokenized6.values])\n",
    "\n",
    "np.array(padded6).shape        # Dimensions of the padded variable        \n",
    "\n",
    "attention_mask6 = np.where(padded6 != 0, 1, 0)\n",
    "attention_mask6.shape\n",
    "input_ids6 = torch.tensor(padded6)  \n",
    "attention_mask6 = torch.tensor(attention_mask6)\n",
    "input_segments6= np.array([_get_segments3(token, max_len6)for token in tokenized6.values])\n",
    "token_type_ids6 = torch.tensor(input_segments6)\n",
    "input_segments6 = torch.tensor(input_segments6)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states6 = model(input_ids6, attention_mask=attention_mask6, token_type_ids=input_segments6)   \n",
    "features6 = last_hidden_states6[0][:,0,:].numpy()\n",
    "features6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c4d7af-0640-427d-9d50-bc99ea9afe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#**df7**\"\"\"\n",
    "\n",
    "pair7=df7['Title1'] + df7['Description1']+ [\" [SEP] \"] + df7['Title2'] + df7['Description2']\n",
    "tokenized7 = pair7.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))\n",
    "\n",
    "max_len7 = 0                 # padding all lists to the same size\n",
    "for i in tokenized7.values:\n",
    "    if len(i) > max_len7:\n",
    "        max_len7 = len(i)\n",
    "\n",
    "max_len7=300\n",
    "padded7 = np.array([i + [0]*(max_len7-len(i)) for i in tokenized7.values])\n",
    "\n",
    "np.array(padded7).shape        # Dimensions of the padded variable    \n",
    "\n",
    "attention_mask7 = np.where(padded7 != 0, 1, 0)\n",
    "attention_mask7.shape\n",
    "input_ids7 = torch.tensor(padded7)  \n",
    "attention_mask7 = torch.tensor(attention_mask7)\n",
    "input_segments7= np.array([_get_segments3(token, max_len7)for token in tokenized7.values])\n",
    "token_type_ids7 = torch.tensor(input_segments7)\n",
    "input_segments7 = torch.tensor(input_segments7)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states7 = model(input_ids7, attention_mask=attention_mask7, token_type_ids=input_segments7)  \n",
    "features7 = last_hidden_states7[0][:,0,:].numpy()\n",
    "features7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2030360-6ed1-485e-aeed-d91a5fbcd5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#**df8**\"\"\"\n",
    "\n",
    "pair8=df8['Title1'] + df8['Description1']+ [\" [SEP] \"] + df8['Title2'] + df8['Description2']\n",
    "tokenized8 = pair8.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))\n",
    "\n",
    "max_len8 = 0                 # padding all lists to the same size\n",
    "for i in tokenized8.values:\n",
    "    if len(i) > max_len8:\n",
    "        max_len8 = len(i)\n",
    "max_len8=300\n",
    "padded8 = np.array([i + [0]*(max_len8-len(i)) for i in tokenized8.values])\n",
    "\n",
    "np.array(padded8).shape        # Dimensions of the padded variable  \n",
    "\n",
    "\n",
    "attention_mask8 = np.where(padded8 != 0, 1, 0)\n",
    "attention_mask8.shape\n",
    "input_ids8 = torch.tensor(padded8)  \n",
    "attention_mask8 = torch.tensor(attention_mask8)\n",
    "input_segments8= np.array([_get_segments3(token, max_len8)for token in tokenized8.values])\n",
    "token_type_ids8 = torch.tensor(input_segments8)\n",
    "input_segments8 = torch.tensor(input_segments8)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states8 = model(input_ids8, attention_mask=attention_mask8, token_type_ids=input_segments8)   \n",
    "features8 = last_hidden_states8[0][:,0,:].numpy()\n",
    "features8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e2ac03-8439-4316-8746-456a620db060",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#**df9**\"\"\"\n",
    "\n",
    "pair9=df9['Title1'] + df9['Description1']+ [\" [SEP] \"] + df9['Title2'] + df9['Description2']\n",
    "tokenized9 = pair9.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))\n",
    "\n",
    "max_len9 = 0                 # padding all lists to the same size\n",
    "for i in tokenized9.values:\n",
    "    if len(i) > max_len9:\n",
    "        max_len9 = len(i)\n",
    "max_len9=300\n",
    "padded9 = np.array([i + [0]*(max_len9-len(i)) for i in tokenized9.values])\n",
    "\n",
    "np.array(padded9).shape        # Dimensions of the padded variable    \n",
    "\n",
    "attention_mask9 = np.where(padded9 != 0, 1, 0)\n",
    "attention_mask9.shape\n",
    "input_ids9 = torch.tensor(padded9)  \n",
    "attention_mask9 = torch.tensor(attention_mask9)\n",
    "input_segments9= np.array([_get_segments3(token, max_len9)for token in tokenized9.values])\n",
    "token_type_ids9 = torch.tensor(input_segments9)\n",
    "input_segments9 = torch.tensor(input_segments9)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states9 = model(input_ids9, attention_mask=attention_mask9, token_type_ids=input_segments9)    \n",
    "features9 = last_hidden_states9[0][:,0,:].numpy()\n",
    "features9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e223729c-d966-497e-881f-0ec79c0e3839",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#**df10**\"\"\"\n",
    "\n",
    "pair10=df10['Title1'] + df10['Description1']+ [\" [SEP] \"] + df10['Title2'] + df10['Description2']\n",
    "tokenized10 = pair10.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))\n",
    "max_len10 = 0                 # padding all lists to the same size\n",
    "for i in tokenized10.values:\n",
    "    if len(i) > max_len10:\n",
    "        max_len10 = len(i)\n",
    "max_len10=300\n",
    "padded10 = np.array([i + [0]*(max_len10-len(i)) for i in tokenized10.values])\n",
    "\n",
    "np.array(padded10).shape        # Dimensions of the padded variable\n",
    "\n",
    "attention_mask10 = np.where(padded10 != 0, 1, 0)\n",
    "attention_mask10.shape\n",
    "input_ids10 = torch.tensor(padded10)  \n",
    "attention_mask10 = torch.tensor(attention_mask10)\n",
    "input_segments10= np.array([_get_segments3(token, max_len10)for token in tokenized10.values])\n",
    "token_type_ids10 = torch.tensor(input_segments10)\n",
    "input_segments10 = torch.tensor(input_segments10)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states10 = model(input_ids10, attention_mask=attention_mask10, token_type_ids=input_segments10) \n",
    "features10 = last_hidden_states10[0][:,0,:].numpy()\n",
    "features10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904da261-4a34-4556-9e46-299003adbea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#**df11**\"\"\"\n",
    "\n",
    "pair11=df11['Title1'] + df11['Description1']+ [\" [SEP] \"] + df11['Title2'] + df11['Description2']\n",
    "tokenized11 = pair11.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))\n",
    "\n",
    "max_len11 = 0                 # padding all lists to the same size\n",
    "for i in tokenized11.values:\n",
    "    if len(i) > max_len11:\n",
    "        max_len11 = len(i)\n",
    "max_len11=300\n",
    "padded11 = np.array([i + [0]*(max_len11-len(i)) for i in tokenized11.values])\n",
    "\n",
    "np.array(padded11).shape        # Dimensions of the padded variable   \n",
    "\n",
    "attention_mask11 = np.where(padded11 != 0, 1, 0)\n",
    "attention_mask11.shape\n",
    "input_ids11 = torch.tensor(padded11)  \n",
    "attention_mask11 = torch.tensor(attention_mask11)\n",
    "input_segments11= np.array([_get_segments3(token, max_len11)for token in tokenized11.values])\n",
    "token_type_ids11 = torch.tensor(input_segments11)\n",
    "input_segments11 = torch.tensor(input_segments11)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states11 = model(input_ids11, attention_mask=attention_mask11, token_type_ids=input_segments11)   \n",
    "features11 = last_hidden_states11[0][:,0,:].numpy()\n",
    "features11\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8a575b-a1ca-4b7b-a66a-0f6b83d4afd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#**Classification**\"\"\"\n",
    "\n",
    "features=np.concatenate([features3,features4,features5,features6,features7,features8,features9,features10,features11,features12,features13,features14,features15,features16,features17,features18,features19,features20,features21,features22,features23,features24,features25,features26,features27,features29, features30, features31])\n",
    "\n",
    "#features=np.concatenate([features3,features4,features5,features6,features7,features8,features9,features10,features11])\n",
    "\n",
    "features.shape\n",
    "\n",
    "Total = pd.concat([df3,df4,df5,df6,df7,df8,df9,df10,df11], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "labels =Total['Label']\n",
    "labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ffaaee-35cd-4bb5-9774-1ba4615f49fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"hold out \"\"\"\n",
    "\n",
    "# for thunderbird\n",
    "train_features = features[0:6792]\n",
    "train_labels = labels[0:6792]\n",
    "test_features = features[6792:]\n",
    "test_labels = labels[6792:]\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(max_iter=100)\n",
    "from sklearn.datasets import make_classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4168fca4-be0e-476c-bf2f-3aa39e53f4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# **MLP Classifier**\"\"\"\n",
    "\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(50,100,50), (50,100,50), (100,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3)\n",
    "clf.fit(train_features, train_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b0bd09-f27d-4528-96e4-7aba50608d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best paramete set\n",
    "print('Best parameters found:\\n', clf.best_params_)\n",
    "\n",
    "# All results\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "\n",
    "\n",
    "\n",
    "y_true, y_pred = test_labels , clf.predict(test_features)\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bba4cd-41da-49e4-bc96-71459d4136f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print('Results on the test set:')\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(confusion_matrix(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efa4554-f60f-413a-970b-ea9c6b8ff56f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dddd7c-35c1-4bd1-aa9e-1878be1de395",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
