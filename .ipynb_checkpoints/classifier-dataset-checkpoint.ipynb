{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4b27da2-abaf-4203-ac50-cd24176c0d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (4.44.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from transformers) (0.24.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from requests->transformers) (2024.12.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febed13c-32d5-4a51-a26c-7849803f354e",
   "metadata": {},
   "source": [
    "\"\"\"##**Importing the tools**\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d26d3afb-6218-471e-a12f-56e7177781d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch\n",
    "import transformers as ppb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "import re\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9d289fc-f0b4-40a5-80cf-a4d19317abac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thunderbird\n",
    "df1=pd.read_csv(r'E:\\PycharmProjects\\duplicate bug report detection\\Duplicate Bug Report\\ThunderBird\\dup_TB.csv',delimiter=';')\n",
    "df2=pd.read_csv(r'E:\\PycharmProjects\\duplicate bug report detection\\Duplicate Bug Report\\ThunderBird\\Nondup_TB.csv',delimiter=';')\n",
    "\n",
    "df1['Label'] = 'duplicate'\n",
    "df2['Label'] = 'non duplicate'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10f1b6c-2581-44d2-9f45-1e2a7da169f2",
   "metadata": {},
   "source": [
    "#### \\反斜杠可能会被认为转义，在绝对路径之前加上r以表示非转义，来解决SyntaxError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "145b8bfb-a8ce-4a5c-9885-4d4146e3cb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"##**Loading the Pre-trained BERT model**\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84bed39a-265d-46f6-8170-96a480a65d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# **Remove stop words**\"\"\"\n",
    "\n",
    "df1['Title1']= df1['Title1'].str.replace(r'i' 'me' 'my' 'myself' 'we' \n",
    "                                         'our' 'ours' 'ourselves' 'you' 'your' 'yours' 'yourself''yourselves' 'they' 'we' 'him' 'he' 'him' 'his' 'himself' 'she' 'her' 'hers' 'herself' 'it''its' 'itself' 'they' 'them' 'their' 'theirs' 'themselves'  'what' 'which' 'who' 'whom' 'this' 'that'\n",
    "                                         'these' 'those' 'am' 'is' 'are' 'was' 'were' 'be' 'been' 'being' 'have' 'has' 'had' 'having' 'do' 'does' 'did' 'doing' 'a' 'an' 'the' 'and' 'but' 'if' 'or' 'because' 'as' 'until' 'while' 'of' 'at' 'by' 'for' 'with' 'about' 'against' 'between' 'into' 'through' \n",
    "                                         'during' 'before' 'after' 'above' 'below' 'to' 'from' 'up' 'down' 'in' 'out' 'on' 'off' 'over' 'under' 'again' 'further' 'then' 'once'  'here' 'there' 'when' 'where' 'why' 'how' 'all' 'any' 'both' 'each' 'few' 'more' 'most' 'other' 'some' 'such' 'no' 'nor' 'not' 'only' 'own' 'same' 'so' 'than' 'too' 'very' 's' 't' 'can' 'will' 'just' 'don' 'should' 'now'\n",
    "                                         'java' 'com' 'org' ,'')\n",
    "\n",
    "\n",
    "df1['Title2']= df1['Title2'].str.replace(r'i' 'me' 'my' 'myself' 'we' \n",
    "                                         'our' 'ours' 'ourselves' 'you' 'your' 'yours' 'yourself''yourselves' 'they' 'we' 'him' 'he' 'him' 'his' 'himself' 'she' 'her' 'hers' 'herself' 'it''its' 'itself' 'they' 'them' 'their' 'theirs' 'themselves'  'what' 'which' 'who' 'whom' 'this' 'that'\n",
    "                                         'these' 'those' 'am' 'is' 'are' 'was' 'were' 'be' 'been' 'being' 'have' 'has' 'had' 'having' 'do' 'does' 'did' 'doing' 'a' 'an'\n",
    "                                         'the' 'and' 'but' 'if' 'or' 'because' 'as' 'until' 'while' 'of' 'at' 'by' 'for' 'with' 'about' 'against' 'between' 'into' 'through' 'during' 'before' 'after' 'above' 'below' 'to' 'from' 'up' 'down' 'in' 'out' 'on' 'off' 'over' 'under' 'again' 'further' 'then' 'once' 'here' 'there' 'when' 'where' 'why' 'how' 'all' 'any' 'both' 'each' 'few' 'more' 'most' 'other' 'some' 'such' 'no' 'nor' 'not' 'only' 'own' \n",
    "                                         'same' 'so' 'than' 'too' 'very' 's' 't' 'can' 'will' 'just' 'don' 'should' 'now'\n",
    "                                         'java' 'com' 'org' ,'')\n",
    "\n",
    "df2['Title1']= df2['Title1'].str.replace(r'i' 'me' 'my' 'myself' 'we' 'our' 'ours' 'ourselves' 'you' 'your' 'yours' 'yourself''yourselves' 'they' 'we' 'him' 'he' 'him' 'his' 'himself' 'she'\n",
    "                                         'her' 'hers' 'herself' 'it''its' 'itself' 'they' 'them' 'their' 'theirs' 'themselves'  'what' 'which' 'who' 'whom' 'this' 'that' 'these' 'those' 'am' 'is' 'are' 'was' 'were' 'be' 'been' 'being' 'have' 'has' 'had' 'having' 'do' 'does' 'did' 'doing' 'a' 'an'\n",
    "                                         'the' 'and' 'but' 'if' 'or' 'because' 'as' 'until' 'while' 'of' 'at' 'by' 'for' 'with' 'about' 'against' 'between' 'into' 'through' \n",
    "                                         'during' 'before' 'after' 'above' 'below' 'to' 'from' 'up' 'down' 'in' 'out' 'on' 'off' 'over' 'under' 'again' 'further' 'then' 'once' 'here' 'there' 'when' 'where' 'why' 'how' 'all' 'any' 'both' 'each' 'few' 'more' 'most' 'other' 'some' 'such' 'no' 'nor' 'not' 'only' 'own' 'same' 'so' 'than' 'too' 'very' 's' 't' 'can' 'will' 'just' 'don' 'should' 'now'\n",
    "                                         'java' 'com' 'org' ,'')\n",
    "\n",
    "\n",
    "df2['Title2']= df2['Title2'].str.replace(r'i' 'me' 'my' 'myself' 'we' 'our' 'ours' 'ourselves' 'you' 'your' 'yours' 'yourself''yourselves' 'they' 'we' 'him' 'he' 'him' 'his' 'himself' 'she'\n",
    "                                         'her' 'hers' 'herself' 'it''its' 'itself' 'they' 'them' 'their' 'theirs' 'themselves'  'what' 'which' 'who' 'whom' 'this' 'that' 'these' 'those' 'am' 'is' 'are' 'was' 'were' 'be' 'been' 'being' 'have' 'has' 'had' 'having' 'do' 'does' 'did' 'doing' 'a' 'an'\n",
    "                                         'the' 'and' 'but' 'if' 'or' 'because' 'as' 'until' 'while' 'of' 'at' 'by' 'for' 'with' 'about' 'against' 'between' 'into' 'through' \n",
    "                                         'during' 'before' 'after' 'above' 'below' 'to' 'from' 'up' 'down' 'in' 'out' 'on' 'off' 'over' 'under' 'again' 'further' 'then' 'once' 'here' 'there' 'when' 'where' 'why' 'how' 'all' 'any' 'both' 'each' 'few' 'more' 'most' 'other' 'some' 'such' 'no' 'nor' 'not' 'only' 'own' \n",
    "                                         'same' 'so' 'than' 'too' 'very' 's' 't' 'can' 'will' 'just' 'don' 'should' 'now'\n",
    "                                         'java' 'com' 'org' ,'')\n",
    "\n",
    "df1['Description1']= df1['Description1'].str.replace(r'i' 'me' 'my' 'myself' 'we'  'our' 'ours' 'ourselves' 'you' 'your' 'yours' 'yourself''yourselves' 'they' 'we' 'him' 'he' 'him' 'his' 'himself' 'she'\n",
    "                                         'her' 'hers' 'herself' 'it''its' 'itself' 'they' 'them' 'their' 'theirs' 'themselves'  'what' 'which' 'who' 'whom' 'this' 'that' 'these' 'those' 'am' 'is' 'are' 'was' 'were' 'be' 'been' 'being' 'have' 'has' 'had' 'having' 'do' 'does' 'did' 'doing' 'a' 'an' 'the' 'and' 'but' 'if' 'or' 'because' 'as' 'until' 'while' 'of' 'at' 'by' 'for' 'with' 'about' 'against' 'between' 'into' 'through' \n",
    "                                         'during' 'before' 'after' 'above' 'below' 'to' 'from' 'up' 'down' 'in' 'out' 'on' 'off' 'over' 'under' 'again' 'further' 'then' 'once' \n",
    "                                         'here' 'there' 'when' 'where' 'why' 'how' 'all' 'any' 'both' 'each' 'few' 'more' 'most' 'other' 'some' 'such' 'no' 'nor' 'not' 'only' 'own' 'same' 'so' 'than' 'too' 'very' 's' 't' 'can' 'will' 'just' 'don' 'should' 'now'\n",
    "                                         'java' 'com' 'org' ,'')\n",
    "\n",
    "df1['Description2']= df1['Description2'].str.replace(r'i' 'me' 'my' 'myself' 'we' 'our' 'ours' 'ourselves' 'you' 'your' 'yours' 'yourself''yourselves' 'they' 'we' 'him' 'he' 'him' 'his' 'himself' 'she'\n",
    "                                         'her' 'hers' 'herself' 'it''its' 'itself' 'they' 'them' 'their' 'theirs' 'themselves'  'what' 'which' 'who' 'whom' 'this' 'that' 'these' 'those' 'am' 'is' 'are' 'was' 'were' 'be' 'been' 'being' 'have' 'has' 'had' 'having' 'do' 'does' 'did' 'doing' 'a' 'an'\n",
    "                                         'the' 'and' 'but' 'if' 'or' 'because' 'as' 'until' 'while' 'of' 'at' 'by' 'for' 'with' 'about' 'against' 'between' 'into' 'through' \n",
    "                                         'during' 'before' 'after' 'above' 'below' 'to' 'from' 'up' 'down' 'in' 'out' 'on' 'off' 'over' 'under' 'again' 'further' 'then' 'once' 'here' 'there' 'when' 'where' 'why' 'how' 'all' 'any' 'both' 'each' 'few' 'more' 'most' 'other' 'some' 'such' 'no' 'nor' 'not' 'only' 'own' \n",
    "                                         'same' 'so' 'than' 'too' 'very' 's' 't' 'can' 'will' 'just' 'don' 'should' 'now'\n",
    "                                         'java' 'com' 'org' ,'')\n",
    "\n",
    "df2['Description1']= df2['Description1'].str.replace(r'i' 'me' 'my' 'myself' 'we' 'our' 'ours' 'ourselves' 'you' 'your' 'yours' 'yourself''yourselves' 'they' 'we' 'him' 'he' 'him' 'his' 'himself' 'she'\n",
    "                                         'her' 'hers' 'herself' 'it''its' 'itself' 'they' 'them' 'their' 'theirs' 'themselves'  'what' 'which' 'who' 'whom' 'this' 'that' 'these' 'those' 'am' 'is' 'are' 'was' 'were' 'be' 'been' 'being' 'have' 'has' 'had' 'having' 'do' 'does' 'did' 'doing' 'a' 'an'\n",
    "                                         'the' 'and' 'but' 'if' 'or' 'because' 'as' 'until' 'while' 'of' 'at' 'by' 'for' 'with' 'about' 'against' 'between' 'into' 'through' 'during' 'before' 'after' 'above' 'below' 'to' 'from' 'up' 'down' 'in' 'out' 'on' 'off' 'over' 'under' 'again' 'further' 'then' 'once' \n",
    "                                         'here' 'there' 'when' 'where' 'why' 'how' 'all' 'any' 'both' 'each' 'few' 'more' 'most' 'other' 'some' 'such' 'no' 'nor' 'not' 'only' 'own' 'same' 'so' 'than' 'too' 'very' 's' 't' 'can' 'will' 'just' 'don' 'should' 'now'\n",
    "                                         'java' 'com' 'org' ,'')\n",
    "\n",
    "df2['Description2']= df2['Description2'].str.replace(r'i' 'me' 'my' 'myself' 'we' 'our' 'ours' 'ourselves' 'you' 'your' 'yours' 'yourself''yourselves' 'they' 'we' 'him' 'he' 'him' 'his' 'himself' 'she'\n",
    "                                         'her' 'hers' 'herself' 'it''its' 'itself' 'they' 'them' 'their' 'theirs' 'themselves'  'what' 'which' 'who' 'whom' 'this' 'that' 'these' 'those' 'am' 'is' 'are' 'was' 'were' 'be' 'been' 'being' 'have' 'has' 'had' 'having' 'do' 'does' 'did' 'doing' 'a' 'an'\n",
    "                                         'the' 'and' 'but' 'if' 'or' 'because' 'as' 'until' 'while' 'of' 'at' 'by' 'for' 'with' 'about' 'against' 'between' 'into' 'through' 'during' 'before' 'after' 'above' 'below' 'to' 'from' 'up' 'down' 'in' 'out' 'on' 'off' 'over' 'under' 'again' 'further' 'then' 'once' \n",
    "                                         'here' 'there' 'when' 'where' 'why' 'how' 'all' 'any' 'both' 'each' 'few' 'more' 'most' 'other' 'some' 'such' 'no' 'nor' 'not' 'only' 'own'  'same' 'so' 'than' 'too' 'very' 's' 't' 'can' 'will' 'just' 'don' 'should' 'now'\n",
    "                                         'java' 'com' 'org' ,'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d5a7ad2-856d-4220-85c2-57e5cdc02a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#**Batch ThunderBird**\"\"\"\n",
    "\n",
    "batch_31=df1[:500]\n",
    "batch_32=df2[:500]\n",
    "df3 = pd.concat([batch_31,batch_32], ignore_index=True)\n",
    "batch_41=df1[500:1000]\n",
    "batch_42=df2[500:1000]\n",
    "df4 = pd.concat([batch_41,batch_42], ignore_index=True)\n",
    "batch_51=df1[1000:1500]\n",
    "batch_52=df2[1000:1500]\n",
    "df5 = pd.concat([batch_51,batch_52], ignore_index=True)\n",
    "batch_61=df1[1500:2000]\n",
    "batch_62=df2[1500:2000]\n",
    "df6 = pd.concat([batch_61,batch_62], ignore_index=True)\n",
    "batch_71=df1[2000:2500]\n",
    "batch_72=df2[2000:2500]\n",
    "df7 = pd.concat([batch_71,batch_72], ignore_index=True)\n",
    "batch_81=df1[2500:3000]\n",
    "batch_82=df2[2500:3000]\n",
    "df8 = pd.concat([batch_81,batch_82], ignore_index=True)\n",
    "batch_91=df1[3000:3486]\n",
    "batch_92=df2[3000:3486]\n",
    "df9 = pd.concat([batch_91,batch_92], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9edda4d8-ab81-424e-af98-1f8096d64052",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing\n",
    "batch_101=df1[3486:3900]\n",
    "batch_102=df2[3486:3900]\n",
    "df10 = pd.concat([batch_101,batch_102], ignore_index=True)\n",
    "batch_111=df1[3900:4338]\n",
    "batch_112=df2[3900:4374]\n",
    "df11 = pd.concat([batch_111,batch_112], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877326a4-c18b-41e9-801d-0b18599992a8",
   "metadata": {},
   "source": [
    "### **_get_segments3**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1201a414-0b75-402d-a80d-ef69e1ede779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_segments3(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    segments = []\n",
    "    first_sep = False\n",
    "    current_segment_id = 0 \n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        #print(token)\n",
    "        if token == 102:\n",
    "            #if first_sep:\n",
    "                #first_sep = False \n",
    "            #else:\n",
    "           current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "553d7d35-be59-4062-9c5f-0ca6a12f1afe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.91255605, -0.21790832, -0.5227412 , ..., -0.7319654 ,\n",
       "         0.36339873,  0.4154484 ],\n",
       "       [-0.9533658 , -0.0871788 , -0.45151824, ..., -0.77327883,\n",
       "         0.08480891,  0.56058335],\n",
       "       [-1.1020757 ,  0.00908023, -0.8869153 , ..., -1.1422946 ,\n",
       "         0.35178795,  0.5118283 ],\n",
       "       ...,\n",
       "       [-0.8755185 , -0.71465635, -0.00558183, ..., -0.16154872,\n",
       "        -0.340501  ,  0.66999435],\n",
       "       [-0.43209928, -0.53146505, -0.44968665, ..., -0.47059727,\n",
       "        -0.13234805,  0.750314  ],\n",
       "       [-0.8891295 , -0.5272192 , -0.34446254, ..., -0.8459486 ,\n",
       "         0.175968  ,  0.26481915]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"#**df3**\"\"\"\n",
    "\n",
    "pair3= df3['Title1'] + df3['Description1']+ [\" [SEP] \"] + df3['Title2'] + df3['Description2']\n",
    "tokenized3 = pair3.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))\n",
    "\n",
    "max_len3 = 0                 # padding all lists to the same size\n",
    "for i in tokenized3.values:\n",
    "    if len(i) > max_len3:\n",
    "        max_len3 = len(i)\n",
    "max_len3 =300\n",
    "padded3 = np.array([i + [0]*(max_len3-len(i)) for i in tokenized3.values])\n",
    "\n",
    "np.array(padded3).shape\n",
    "\n",
    "attention_mask3 = np.where(padded3 != 0, 1, 0)\n",
    "attention_mask3.shape\n",
    "input_ids3 = torch.tensor(padded3)  \n",
    "attention_mask3 = torch.tensor(attention_mask3)\n",
    "input_segments3= np.array([_get_segments3(token, max_len3)for token in tokenized3.values])\n",
    "token_type_ids3 = torch.tensor(input_segments3)\n",
    "input_segments3 = torch.tensor(input_segments3)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states3 = model(input_ids3, attention_mask=attention_mask3, token_type_ids=input_segments3)    # <<< 600 rows only !!!\n",
    "features3 = last_hidden_states3[0][:,0,:].numpy()\n",
    "features3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96735e25-4041-4bc7-be8f-711209c67d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.0030321 , -0.3101899 , -0.4147074 , ..., -0.41361225,\n",
       "         0.08032121,  0.58812845],\n",
       "       [-0.57388115, -0.1158052 ,  0.29708412, ..., -0.4643554 ,\n",
       "         0.45689783,  0.6111775 ],\n",
       "       [-0.5566804 , -0.13495302, -0.61258477, ..., -0.74251866,\n",
       "         0.45100045,  0.41690361],\n",
       "       ...,\n",
       "       [-0.7471177 , -0.22713074, -0.16769981, ..., -0.8237756 ,\n",
       "        -0.03927189,  0.47090805],\n",
       "       [-0.599951  , -0.19942358, -0.02866199, ..., -0.39740124,\n",
       "         0.13446511,  0.38081968],\n",
       "       [-0.25177166, -0.2654702 ,  0.0568625 , ..., -0.7201651 ,\n",
       "         0.39396408,  0.441244  ]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"#**df4**\"\"\"\n",
    "\n",
    "pair4=df4['Title1'] + df4['Description1']+ [\" [SEP] \"] + df4['Title2']  + df4['Description2']\n",
    "tokenized4 = pair4.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))\n",
    "max_len4 = 0                 # padding all lists to the same size\n",
    "for i in tokenized4.values:\n",
    "    if len(i) > max_len4:\n",
    "        max_len4 = len(i)\n",
    "max_len4 =300\n",
    "padded4 = np.array([i + [0]*(max_len4-len(i)) for i in tokenized4.values])\n",
    "\n",
    "np.array(padded4).shape\n",
    "\n",
    "attention_mask4 = np.where(padded4 != 0, 1, 0)\n",
    "attention_mask4.shape\n",
    "input_ids4 = torch.tensor(padded4)  \n",
    "attention_mask4 = torch.tensor(attention_mask4)\n",
    "input_segments4= np.array([_get_segments3(token, max_len4)for token in tokenized4.values])\n",
    "token_type_ids4 = torch.tensor(input_segments4)\n",
    "input_segments4 = torch.tensor(input_segments4)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states4 = model(input_ids4, attention_mask=attention_mask4, token_type_ids=input_segments4)   \n",
    "features4 = last_hidden_states4[0][:,0,:].numpy()\n",
    "features4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "026a2704-5621-4630-b938-e551f748a6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#**df5**\"\"\"\n",
    "\n",
    "pair5=df5['Title1'] + df5['Description1']+ [\" [SEP] \"] + df5['Title2'] + df5['Description2']\n",
    "tokenized5 = pair5.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d150551b-984c-433e-9326-412a74ee5b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 300)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"##**Padding**\"\"\"\n",
    "\n",
    "max_len5 = 0                 # padding all lists to the same size\n",
    "for i in tokenized5.values:\n",
    "    if len(i) > max_len5:\n",
    "        max_len5 = len(i)\n",
    "\n",
    "max_len5 =300\n",
    "padded5 = np.array([i + [0]*(max_len5-len(i)) for i in tokenized5.values])\n",
    "\n",
    "np.array(padded5).shape        # Dimensions of the padded variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a908a88c-23dd-408e-9694-d3f01d5830d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"##**Masking**\"\"\"\n",
    "\n",
    "attention_mask5 = np.where(padded5 != 0, 1, 0)\n",
    "attention_mask5.shape\n",
    "input_ids5 = torch.tensor(padded5)  \n",
    "attention_mask5 = torch.tensor(attention_mask5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfaddaed-61ad-41e3-bf6d-7dc3cfd8418d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##**Running the `model()` function through BERT**\"\"\"\n",
    "\n",
    "input_segments5= np.array([_get_segments3(token, max_len5)for token in tokenized5.values])\n",
    "token_type_ids5 = torch.tensor(input_segments5)\n",
    "input_segments5 = torch.tensor(input_segments5)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states5 = model(input_ids5, attention_mask=attention_mask5, token_type_ids=input_segments5)    # <<< 600 rows only !!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "213ea7e4-dbe8-41af-9c83-15ef737681b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.8057164 , -0.1903756 , -0.5531031 , ..., -0.81358856,\n",
       "         0.5090866 ,  0.3438378 ],\n",
       "       [-0.99893624, -0.24768645, -0.5971862 , ..., -1.0160621 ,\n",
       "         0.3329332 ,  0.29440314],\n",
       "       [-0.6406629 ,  0.0654166 , -0.51517516, ..., -0.7167264 ,\n",
       "         0.20800443,  0.5322832 ],\n",
       "       ...,\n",
       "       [-0.84924304, -0.04279939, -0.42969108, ..., -0.8743309 ,\n",
       "         0.65776217,  0.6068168 ],\n",
       "       [-0.39133966,  0.21447241, -0.13611522, ..., -0.83492976,\n",
       "         0.5022382 ,  0.38481703],\n",
       "       [-0.6098051 ,  0.06449075,  0.03130186, ..., -0.7901553 ,\n",
       "         0.14110261,  0.9117195 ]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"##**Slicing the part of the output of BERT : [cls]**\"\"\"\n",
    "\n",
    "features5 = last_hidden_states5[0][:,0,:].numpy()\n",
    "features5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5014378e-71c9-4952-ba98-7329d10eda45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.1361568 , -0.05817575, -0.5973464 , ..., -0.9217684 ,\n",
       "         0.03773895,  0.33720908],\n",
       "       [-1.1947366 , -0.2991371 , -0.37157923, ..., -0.9580795 ,\n",
       "         0.10087218,  0.2349794 ],\n",
       "       [-0.8652864 , -0.03457035, -0.34951815, ..., -0.80570906,\n",
       "         0.13683805,  0.20355551],\n",
       "       ...,\n",
       "       [ 0.04832708, -0.6024167 ,  0.13346554, ..., -0.61913806,\n",
       "         0.30246162,  0.66296744],\n",
       "       [-0.9401006 , -0.10338303,  0.05624962, ..., -0.8005834 ,\n",
       "         0.39867064,  0.7059028 ],\n",
       "       [-0.86830163, -0.2280246 , -0.8593748 , ..., -0.77878755,\n",
       "         0.39037374,  0.5396223 ]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"#**df6**\"\"\"\n",
    "\n",
    "pair6=df6['Title1'] + df6['Description1']+ [\" [SEP] \"] + df6['Title2'] + df6['Description2']\n",
    "tokenized6 = pair6.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))\n",
    "\n",
    "max_len6 = 0                 # padding all lists to the same size\n",
    "for i in tokenized6.values:\n",
    "    if len(i) > max_len6:\n",
    "        max_len6 = len(i)\n",
    "\n",
    "max_len6=300\n",
    "padded6 = np.array([i + [0]*(max_len6-len(i)) for i in tokenized6.values])\n",
    "\n",
    "np.array(padded6).shape        # Dimensions of the padded variable        \n",
    "\n",
    "attention_mask6 = np.where(padded6 != 0, 1, 0)\n",
    "attention_mask6.shape\n",
    "input_ids6 = torch.tensor(padded6)  \n",
    "attention_mask6 = torch.tensor(attention_mask6)\n",
    "input_segments6= np.array([_get_segments3(token, max_len6)for token in tokenized6.values])\n",
    "token_type_ids6 = torch.tensor(input_segments6)\n",
    "input_segments6 = torch.tensor(input_segments6)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states6 = model(input_ids6, attention_mask=attention_mask6, token_type_ids=input_segments6)   \n",
    "features6 = last_hidden_states6[0][:,0,:].numpy()\n",
    "features6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8c70405-1dae-4e21-b203-bdff15ec2173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.6701001 , -0.4891559 ,  0.10609879, ..., -0.4559685 ,\n",
       "         0.23931633,  0.7661283 ],\n",
       "       [-1.2892051 , -0.18732953, -0.2633507 , ..., -0.9788446 ,\n",
       "         0.39981645,  0.5773016 ],\n",
       "       [-0.91039526,  0.24073344, -0.6451308 , ..., -0.85921943,\n",
       "         0.49460414,  0.3900222 ],\n",
       "       ...,\n",
       "       [-0.87652373, -0.02859857, -0.5674486 , ..., -0.7936854 ,\n",
       "         0.29684925,  0.5385502 ],\n",
       "       [-0.2754479 , -1.1373811 ,  0.40167063, ..., -0.41518864,\n",
       "        -0.4804711 ,  0.5601159 ],\n",
       "       [-0.76231253, -0.8566641 ,  0.03711113, ..., -0.23024684,\n",
       "        -0.15705192,  0.7461712 ]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"#**df7**\"\"\"\n",
    "\n",
    "pair7=df7['Title1'] + df7['Description1']+ [\" [SEP] \"] + df7['Title2'] + df7['Description2']\n",
    "tokenized7 = pair7.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))\n",
    "\n",
    "max_len7 = 0                 # padding all lists to the same size\n",
    "for i in tokenized7.values:\n",
    "    if len(i) > max_len7:\n",
    "        max_len7 = len(i)\n",
    "\n",
    "max_len7=300\n",
    "padded7 = np.array([i + [0]*(max_len7-len(i)) for i in tokenized7.values])\n",
    "\n",
    "np.array(padded7).shape        # Dimensions of the padded variable    \n",
    "\n",
    "attention_mask7 = np.where(padded7 != 0, 1, 0)\n",
    "attention_mask7.shape\n",
    "input_ids7 = torch.tensor(padded7)  \n",
    "attention_mask7 = torch.tensor(attention_mask7)\n",
    "input_segments7= np.array([_get_segments3(token, max_len7)for token in tokenized7.values])\n",
    "token_type_ids7 = torch.tensor(input_segments7)\n",
    "input_segments7 = torch.tensor(input_segments7)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states7 = model(input_ids7, attention_mask=attention_mask7, token_type_ids=input_segments7)  \n",
    "features7 = last_hidden_states7[0][:,0,:].numpy()\n",
    "features7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d66e5d3f-b5d1-43ac-8250-c16c86c588b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.6869296 , -0.03409891, -0.226767  , ..., -0.6081477 ,\n",
       "         0.02498405,  0.63978684],\n",
       "       [-0.47913465,  0.03688138, -0.00773755, ..., -0.47602078,\n",
       "         0.14905486,  0.6084148 ],\n",
       "       [-1.4320973 , -0.13379821, -0.9898069 , ..., -0.9709378 ,\n",
       "         0.28201357,  0.4759271 ],\n",
       "       ...,\n",
       "       [ 0.09301577, -0.799205  , -0.08968051, ..., -0.45282766,\n",
       "        -0.08543544,  0.39163432],\n",
       "       [-1.5294615 , -0.48197284, -1.0708833 , ..., -1.0606185 ,\n",
       "        -0.31972194,  0.3899912 ],\n",
       "       [-0.9239903 , -0.27469826, -0.9677012 , ..., -1.170199  ,\n",
       "        -0.1080831 ,  0.5025123 ]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"#**df8**\"\"\"\n",
    "\n",
    "pair8=df8['Title1'] + df8['Description1']+ [\" [SEP] \"] + df8['Title2'] + df8['Description2']\n",
    "tokenized8 = pair8.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))\n",
    "\n",
    "max_len8 = 0                 # padding all lists to the same size\n",
    "for i in tokenized8.values:\n",
    "    if len(i) > max_len8:\n",
    "        max_len8 = len(i)\n",
    "max_len8=300\n",
    "padded8 = np.array([i + [0]*(max_len8-len(i)) for i in tokenized8.values])\n",
    "\n",
    "np.array(padded8).shape        # Dimensions of the padded variable  \n",
    "\n",
    "\n",
    "attention_mask8 = np.where(padded8 != 0, 1, 0)\n",
    "attention_mask8.shape\n",
    "input_ids8 = torch.tensor(padded8)  \n",
    "attention_mask8 = torch.tensor(attention_mask8)\n",
    "input_segments8= np.array([_get_segments3(token, max_len8)for token in tokenized8.values])\n",
    "token_type_ids8 = torch.tensor(input_segments8)\n",
    "input_segments8 = torch.tensor(input_segments8)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states8 = model(input_ids8, attention_mask=attention_mask8, token_type_ids=input_segments8)   \n",
    "features8 = last_hidden_states8[0][:,0,:].numpy()\n",
    "features8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7dc5184d-8cbc-4986-828b-c528ef206f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.1437858 , -0.23147327, -0.7149345 , ..., -1.2486683 ,\n",
       "         0.03549907,  0.78992647],\n",
       "       [-0.6263066 ,  0.1480324 , -0.7074985 , ..., -0.62794995,\n",
       "         0.26492113,  0.4628313 ],\n",
       "       [-0.7451761 ,  0.3735841 , -0.20779136, ..., -0.9193807 ,\n",
       "         0.48205826,  0.51828074],\n",
       "       ...,\n",
       "       [-0.9390641 , -0.19940075, -0.5320647 , ..., -0.70878124,\n",
       "         0.05633758,  0.32540512],\n",
       "       [-0.5181735 , -0.2055099 ,  0.31247526, ..., -0.26686597,\n",
       "        -0.18393773,  0.70344025],\n",
       "       [-0.95971346, -0.5075119 , -0.01881169, ..., -0.430428  ,\n",
       "        -0.03513318,  0.29017213]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"#**df9**\"\"\"\n",
    "\n",
    "pair9=df9['Title1'] + df9['Description1']+ [\" [SEP] \"] + df9['Title2'] + df9['Description2']\n",
    "tokenized9 = pair9.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))\n",
    "\n",
    "max_len9 = 0                 # padding all lists to the same size\n",
    "for i in tokenized9.values:\n",
    "    if len(i) > max_len9:\n",
    "        max_len9 = len(i)\n",
    "max_len9=300\n",
    "padded9 = np.array([i + [0]*(max_len9-len(i)) for i in tokenized9.values])\n",
    "\n",
    "np.array(padded9).shape        # Dimensions of the padded variable    \n",
    "\n",
    "attention_mask9 = np.where(padded9 != 0, 1, 0)\n",
    "attention_mask9.shape\n",
    "input_ids9 = torch.tensor(padded9)  \n",
    "attention_mask9 = torch.tensor(attention_mask9)\n",
    "input_segments9= np.array([_get_segments3(token, max_len9)for token in tokenized9.values])\n",
    "token_type_ids9 = torch.tensor(input_segments9)\n",
    "input_segments9 = torch.tensor(input_segments9)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states9 = model(input_ids9, attention_mask=attention_mask9, token_type_ids=input_segments9)    \n",
    "features9 = last_hidden_states9[0][:,0,:].numpy()\n",
    "features9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "560f0b3c-f716-4843-9f80-411bfb207bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.3166767 ,  0.05735008, -0.9648578 , ..., -0.8426229 ,\n",
       "         0.17924395,  0.5238281 ],\n",
       "       [-0.2967026 ,  0.25737566,  0.33133754, ..., -0.3222556 ,\n",
       "         0.273611  ,  0.24694155],\n",
       "       [-1.039318  , -0.06995827, -0.8155348 , ..., -0.61375135,\n",
       "         0.17017902,  0.3832465 ],\n",
       "       ...,\n",
       "       [-0.7544489 , -0.32525358, -0.1915146 , ..., -0.77948457,\n",
       "        -0.08473228,  0.74177325],\n",
       "       [-0.43942648, -0.45400065, -0.289562  , ..., -0.66389084,\n",
       "         0.48571783,  0.19287752],\n",
       "       [-0.6013321 , -0.12632683, -0.18347684, ..., -0.67152274,\n",
       "        -0.20840523,  0.7867042 ]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"#**df10**\"\"\"\n",
    "\n",
    "pair10=df10['Title1'] + df10['Description1']+ [\" [SEP] \"] + df10['Title2'] + df10['Description2']\n",
    "tokenized10 = pair10.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))\n",
    "max_len10 = 0                 # padding all lists to the same size\n",
    "for i in tokenized10.values:\n",
    "    if len(i) > max_len10:\n",
    "        max_len10 = len(i)\n",
    "max_len10=300\n",
    "padded10 = np.array([i + [0]*(max_len10-len(i)) for i in tokenized10.values])\n",
    "\n",
    "np.array(padded10).shape        # Dimensions of the padded variable\n",
    "\n",
    "attention_mask10 = np.where(padded10 != 0, 1, 0)\n",
    "attention_mask10.shape\n",
    "input_ids10 = torch.tensor(padded10)  \n",
    "attention_mask10 = torch.tensor(attention_mask10)\n",
    "input_segments10= np.array([_get_segments3(token, max_len10)for token in tokenized10.values])\n",
    "token_type_ids10 = torch.tensor(input_segments10)\n",
    "input_segments10 = torch.tensor(input_segments10)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states10 = model(input_ids10, attention_mask=attention_mask10, token_type_ids=input_segments10) \n",
    "features10 = last_hidden_states10[0][:,0,:].numpy()\n",
    "features10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd35387d-e1ce-45e1-b0d2-251f62a23132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.22272   , -0.38673976, -0.6347656 , ..., -0.57898235,\n",
       "        -0.08433632,  0.4182627 ],\n",
       "       [-0.66228014, -0.20980474, -0.0061851 , ..., -0.7366245 ,\n",
       "         0.10534692,  0.589157  ],\n",
       "       [-0.69808346,  0.06130133, -0.47797033, ..., -0.80303043,\n",
       "         0.5648726 ,  0.34571305],\n",
       "       ...,\n",
       "       [-1.028002  , -0.14904141, -0.13647713, ..., -0.83416206,\n",
       "         0.3599294 ,  0.65837365],\n",
       "       [-0.9598469 , -0.24993017,  0.00229281, ..., -0.78532773,\n",
       "         0.37910086,  0.5422846 ],\n",
       "       [-1.0105292 , -0.09327275, -0.8136443 , ..., -0.835127  ,\n",
       "         0.33310658,  0.40924573]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"#**df11**\"\"\"\n",
    "\n",
    "pair11=df11['Title1'] + df11['Description1']+ [\" [SEP] \"] + df11['Title2'] + df11['Description2']\n",
    "tokenized11 = pair11.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))\n",
    "\n",
    "max_len11 = 0                 # padding all lists to the same size\n",
    "for i in tokenized11.values:\n",
    "    if len(i) > max_len11:\n",
    "        max_len11 = len(i)\n",
    "max_len11=300\n",
    "padded11 = np.array([i + [0]*(max_len11-len(i)) for i in tokenized11.values])\n",
    "\n",
    "np.array(padded11).shape        # Dimensions of the padded variable   \n",
    "\n",
    "attention_mask11 = np.where(padded11 != 0, 1, 0)\n",
    "attention_mask11.shape\n",
    "input_ids11 = torch.tensor(padded11)  \n",
    "attention_mask11 = torch.tensor(attention_mask11)\n",
    "input_segments11= np.array([_get_segments3(token, max_len11)for token in tokenized11.values])\n",
    "token_type_ids11 = torch.tensor(input_segments11)\n",
    "input_segments11 = torch.tensor(input_segments11)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states11 = model(input_ids11, attention_mask=attention_mask11, token_type_ids=input_segments11)   \n",
    "features11 = last_hidden_states11[0][:,0,:].numpy()\n",
    "features11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "223ed1f6-dc4c-4b4c-ac35-d302a2f7aa44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           duplicate\n",
       "1           duplicate\n",
       "2           duplicate\n",
       "3           duplicate\n",
       "4           duplicate\n",
       "            ...      \n",
       "8707    non duplicate\n",
       "8708    non duplicate\n",
       "8709    non duplicate\n",
       "8710    non duplicate\n",
       "8711    non duplicate\n",
       "Name: Label, Length: 8712, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"#**Classification**\"\"\"\n",
    "\n",
    "features=np.concatenate([features3,features4,features5,features6,features7,features8,features9,features10,features11])\n",
    "\n",
    "\n",
    "\n",
    "features.shape\n",
    "\n",
    "Total = pd.concat([df3,df4,df5,df6,df7,df8,df9,df10,df11], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "labels =Total['Label']\n",
    "labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c103e60f-fa5f-4814-a0dc-a98e978fade5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"hold out \"\"\"\n",
    "\n",
    "# for thunderbird\n",
    "train_features = features[0:6792]\n",
    "train_labels = labels[0:6792]\n",
    "test_features = features[6792:]\n",
    "test_labels = labels[6792:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba8f826e-6e99-420a-a5d7-82e9a64fdba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(max_iter=100)\n",
    "from sklearn.datasets import make_classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2833394f-3398-4db7-a55a-155b5caf6e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# **MLP Classifier**\"\"\"\n",
    "\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(50,100,50), (50,100,50), (100,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50cd7862-d41a-4716-a700-607099722bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b24d118-0f07-40c9-b4e1-e208b094845d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=3, estimator=MLPClassifier(max_iter=100), n_jobs=-1,\n",
       "             param_grid={&#x27;activation&#x27;: [&#x27;tanh&#x27;, &#x27;relu&#x27;],\n",
       "                         &#x27;alpha&#x27;: [0.0001, 0.05],\n",
       "                         &#x27;hidden_layer_sizes&#x27;: [(50, 100, 50), (50, 100, 50),\n",
       "                                                (100,)],\n",
       "                         &#x27;learning_rate&#x27;: [&#x27;constant&#x27;, &#x27;adaptive&#x27;],\n",
       "                         &#x27;solver&#x27;: [&#x27;sgd&#x27;, &#x27;adam&#x27;]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;GridSearchCV<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>GridSearchCV(cv=3, estimator=MLPClassifier(max_iter=100), n_jobs=-1,\n",
       "             param_grid={&#x27;activation&#x27;: [&#x27;tanh&#x27;, &#x27;relu&#x27;],\n",
       "                         &#x27;alpha&#x27;: [0.0001, 0.05],\n",
       "                         &#x27;hidden_layer_sizes&#x27;: [(50, 100, 50), (50, 100, 50),\n",
       "                                                (100,)],\n",
       "                         &#x27;learning_rate&#x27;: [&#x27;constant&#x27;, &#x27;adaptive&#x27;],\n",
       "                         &#x27;solver&#x27;: [&#x27;sgd&#x27;, &#x27;adam&#x27;]})</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">best_estimator_: MLPClassifier</label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(alpha=0.05, max_iter=100)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;MLPClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(alpha=0.05, max_iter=100)</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=3, estimator=MLPClassifier(max_iter=100), n_jobs=-1,\n",
       "             param_grid={'activation': ['tanh', 'relu'],\n",
       "                         'alpha': [0.0001, 0.05],\n",
       "                         'hidden_layer_sizes': [(50, 100, 50), (50, 100, 50),\n",
       "                                                (100,)],\n",
       "                         'learning_rate': ['constant', 'adaptive'],\n",
       "                         'solver': ['sgd', 'adam']})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3)\n",
    "clf.fit(train_features, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7fe51da-3123-42a6-8d6b-cd0c3fd84fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      " {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "# Best paramete set\n",
    "print('Best parameters found:\\n', clf.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5703d880-0575-48a9-8b83-208c7a880aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.877 (+/-0.019) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.869 (+/-0.019) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.878 (+/-0.013) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.871 (+/-0.020) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.878 (+/-0.021) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.872 (+/-0.028) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.877 (+/-0.017) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.869 (+/-0.017) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.870 (+/-0.020) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.874 (+/-0.030) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.868 (+/-0.018) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.876 (+/-0.027) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.877 (+/-0.020) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.871 (+/-0.015) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.878 (+/-0.018) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.867 (+/-0.019) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.875 (+/-0.017) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.872 (+/-0.025) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.873 (+/-0.017) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.872 (+/-0.021) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.866 (+/-0.017) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.878 (+/-0.018) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.867 (+/-0.018) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.871 (+/-0.038) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.876 (+/-0.015) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.871 (+/-0.019) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.874 (+/-0.011) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.874 (+/-0.020) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.873 (+/-0.017) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.872 (+/-0.027) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.875 (+/-0.017) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.869 (+/-0.018) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.872 (+/-0.018) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.876 (+/-0.019) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.867 (+/-0.017) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.878 (+/-0.024) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.879 (+/-0.017) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.867 (+/-0.024) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.875 (+/-0.020) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.871 (+/-0.029) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.878 (+/-0.018) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.869 (+/-0.022) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.875 (+/-0.014) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.868 (+/-0.019) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.871 (+/-0.017) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.880 (+/-0.027) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.871 (+/-0.022) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.877 (+/-0.019) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "# All results\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f67bcd6-5693-49a5-9ee0-7e017adc4b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['duplicate', 'non duplicate', 'non duplicate', 'duplicate',\n",
       "       'duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'duplicate',\n",
       "       'non duplicate', 'duplicate', 'non duplicate', 'non duplicate',\n",
       "       'duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'duplicate', 'non duplicate', 'duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'duplicate', 'duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'duplicate', 'non duplicate', 'non duplicate', 'duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'duplicate', 'non duplicate', 'non duplicate', 'duplicate',\n",
       "       'non duplicate', 'non duplicate', 'duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'duplicate', 'non duplicate', 'non duplicate',\n",
       "       'duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'duplicate',\n",
       "       'non duplicate', 'non duplicate', 'duplicate', 'non duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'non duplicate', 'duplicate', 'non duplicate', 'duplicate',\n",
       "       'non duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'non duplicate',\n",
       "       'non duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'non duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'non duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'non duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'non duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'non duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'non duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'non duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'non duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'non duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'non duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'non duplicate', 'duplicate', 'non duplicate', 'duplicate',\n",
       "       'non duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'non duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'non duplicate',\n",
       "       'duplicate', 'non duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'non duplicate', 'non duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'non duplicate', 'duplicate', 'non duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'non duplicate', 'non duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'non duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'non duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'non duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'non duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'non duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'non duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'non duplicate', 'duplicate', 'non duplicate', 'duplicate',\n",
       "       'duplicate', 'non duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'non duplicate', 'duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'non duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'non duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'duplicate', 'non duplicate',\n",
       "       'duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'duplicate', 'non duplicate', 'non duplicate', 'duplicate',\n",
       "       'non duplicate', 'duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'duplicate', 'non duplicate', 'duplicate',\n",
       "       'non duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'non duplicate', 'duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'duplicate', 'non duplicate', 'duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'duplicate', 'non duplicate', 'non duplicate', 'duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'duplicate', 'duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'duplicate', 'non duplicate', 'duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'duplicate', 'non duplicate', 'duplicate',\n",
       "       'duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'duplicate', 'non duplicate', 'duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'duplicate',\n",
       "       'non duplicate', 'non duplicate', 'duplicate', 'non duplicate',\n",
       "       'non duplicate', 'duplicate', 'non duplicate', 'duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'duplicate', 'non duplicate', 'duplicate',\n",
       "       'non duplicate', 'duplicate', 'non duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'non duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'non duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'non duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'non duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'non duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'non duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'non duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'non duplicate', 'duplicate', 'duplicate', 'non duplicate',\n",
       "       'duplicate', 'duplicate', 'non duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'non duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'non duplicate',\n",
       "       'duplicate', 'duplicate', 'non duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'non duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'non duplicate',\n",
       "       'non duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'non duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'non duplicate', 'non duplicate',\n",
       "       'duplicate', 'duplicate', 'non duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'non duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'non duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'non duplicate', 'non duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'non duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'non duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'non duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'non duplicate', 'non duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'non duplicate', 'duplicate', 'non duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'non duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'non duplicate', 'duplicate',\n",
       "       'duplicate', 'non duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'non duplicate',\n",
       "       'duplicate', 'duplicate', 'non duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'non duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'non duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'non duplicate', 'duplicate',\n",
       "       'duplicate', 'non duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'non duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'non duplicate', 'non duplicate', 'duplicate', 'duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'non duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'non duplicate', 'duplicate', 'duplicate',\n",
       "       'non duplicate', 'duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'non duplicate', 'duplicate', 'duplicate', 'non duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'non duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'non duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'non duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'non duplicate', 'duplicate',\n",
       "       'duplicate', 'duplicate', 'duplicate', 'duplicate', 'duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'duplicate', 'duplicate', 'non duplicate',\n",
       "       'duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'duplicate', 'non duplicate',\n",
       "       'duplicate', 'non duplicate', 'duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'duplicate', 'non duplicate',\n",
       "       'non duplicate', 'duplicate', 'non duplicate', 'duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'duplicate',\n",
       "       'duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'duplicate', 'duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'duplicate', 'non duplicate', 'non duplicate', 'duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'duplicate', 'duplicate', 'non duplicate',\n",
       "       'non duplicate', 'duplicate', 'duplicate', 'non duplicate',\n",
       "       'non duplicate', 'duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'duplicate', 'duplicate', 'non duplicate',\n",
       "       'non duplicate', 'duplicate', 'non duplicate', 'non duplicate',\n",
       "       'duplicate', 'non duplicate', 'non duplicate', 'duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'duplicate', 'non duplicate', 'duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'duplicate',\n",
       "       'non duplicate', 'duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'duplicate',\n",
       "       'duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'duplicate', 'non duplicate', 'duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'duplicate', 'non duplicate', 'duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'duplicate', 'non duplicate',\n",
       "       'non duplicate', 'duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'non duplicate',\n",
       "       'non duplicate', 'non duplicate', 'non duplicate', 'duplicate',\n",
       "       'duplicate', 'non duplicate'], dtype='<U13')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true, y_pred = test_labels , clf.predict(test_features)\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67fca5f9-2586-48a9-a6bf-b23f85721834",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on the test set:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_true' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report, confusion_matrix\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResults on the test set:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(y_true, y_pred))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(confusion_matrix(y_true, y_pred))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_true' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print('Results on the test set:')\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(test_labels, y_predLr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37c558c0-7052-483d-9224-95873cba5b3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(accuracy_score(test_labels, y_predLr))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_labels' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(test_labels, y_predLr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "933b0d10-e83f-43d0-b9cd-8792cbae7e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters:  {'C': 5.263252631578947}\n",
      "best scrores:  0.8749976429432973\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    duplicate       0.84      0.82      0.83       852\n",
      "non duplicate       0.86      0.87      0.87      1068\n",
      "\n",
      "     accuracy                           0.85      1920\n",
      "    macro avg       0.85      0.85      0.85      1920\n",
      " weighted avg       0.85      0.85      0.85      1920\n",
      "\n",
      "[[702 150]\n",
      " [135 933]]\n",
      "0.8515625\n"
     ]
    }
   ],
   "source": [
    "\"\"\"#**Logistic Regression**\"\"\"\n",
    "\n",
    "parameters = {'C': np.linspace(0.0001, 100, 20)}\n",
    "grid_search = GridSearchCV(LogisticRegression(), parameters, cv=5)\n",
    "grid_search.fit(train_features, train_labels)\n",
    "print('best parameters: ', grid_search.best_params_)\n",
    "print('best scrores: ', grid_search.best_score_)\n",
    "\n",
    "lr_clf = LogisticRegression(C=10.52)\n",
    "lr_clf.fit(train_features, train_labels)\n",
    "\n",
    "lr_clf.score(test_features, test_labels)\n",
    "\n",
    "y_predLr = lr_clf.predict(test_features)\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "y_predLr\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(test_labels,y_predLr))\n",
    "print(confusion_matrix(test_labels, y_predLr))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(test_labels, y_predLr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28a58e15-a603-4c16-8155-f30507ef86f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "    duplicate       0.72      0.77      0.75       852\n",
      "non duplicate       0.81      0.77      0.79      1068\n",
      "\n",
      "     accuracy                           0.77      1920\n",
      "    macro avg       0.76      0.77      0.77      1920\n",
      " weighted avg       0.77      0.77      0.77      1920\n",
      "\n",
      "[[654 198]\n",
      " [249 819]]\n",
      "0.7671875\n"
     ]
    }
   ],
   "source": [
    "\"\"\"#**Decision tree**\"\"\"\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth = 500, random_state = 0)\n",
    "\n",
    "clf.fit(train_features, train_labels)\n",
    "\n",
    "y_preddt = clf.predict(test_features)\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "y_preddt\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(test_labels,y_preddt))\n",
    "print(confusion_matrix(test_labels, y_preddt))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(test_labels, y_preddt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f6971c18-41ad-43fb-86ff-0de489c1714e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[697 155]\n",
      " [136 932]]\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    duplicate       0.84      0.82      0.83       852\n",
      "non duplicate       0.86      0.87      0.86      1068\n",
      "\n",
      "     accuracy                           0.85      1920\n",
      "    macro avg       0.85      0.85      0.85      1920\n",
      " weighted avg       0.85      0.85      0.85      1920\n",
      "\n",
      "0.8484375\n"
     ]
    }
   ],
   "source": [
    "\"\"\"#**SVM**\"\"\"\n",
    "\"\"\" **SVC** \"\"\"\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svclassifier = SVC(kernel='linear')\n",
    "svclassifier.fit(train_features, train_labels)\n",
    "\n",
    "y_pred = svclassifier.predict(test_features)\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "y_pred\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(test_labels,y_pred))\n",
    "print(classification_report(test_labels,y_pred))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d9a32c92-d46b-4522-900a-c564d8be141a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "    duplicate       0.82      0.83      0.83       852\n",
      "non duplicate       0.86      0.86      0.86      1068\n",
      "\n",
      "     accuracy                           0.85      1920\n",
      "    macro avg       0.84      0.84      0.84      1920\n",
      " weighted avg       0.85      0.85      0.85      1920\n",
      "\n",
      "[[709 143]\n",
      " [154 914]]\n",
      "0.8453125\n"
     ]
    }
   ],
   "source": [
    "\"\"\"#**Random Forest**\"\"\"\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=20, random_state=0)\n",
    "rf.fit(train_features, train_labels)\n",
    "y_pred1 = rf.predict(test_features)\n",
    "y_pred1\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(test_labels,y_pred1))\n",
    "print(confusion_matrix(test_labels, y_pred1))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(test_labels, y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9521e550-d8fd-47c4-8fd4-8ac3c59fdc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ffe96a7f-efaf-41c8-95ae-a0a168ca5173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7494791666666667\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    duplicate       0.67      0.84      0.75       852\n",
      "non duplicate       0.84      0.68      0.75      1068\n",
      "\n",
      "     accuracy                           0.75      1920\n",
      "    macro avg       0.76      0.76      0.75      1920\n",
      " weighted avg       0.77      0.75      0.75      1920\n",
      "\n",
      "[[717 135]\n",
      " [346 722]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"**Naive Bayes**\"\"\"\n",
    "# Gaussian\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(train_features, train_labels)\n",
    "\n",
    "y_pred = gnb.predict(test_features)\n",
    "y_pred\n",
    "\n",
    "from sklearn import metrics\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_labels, y_pred))\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(test_labels,y_pred))\n",
    "print(confusion_matrix(test_labels, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "446eec91-f326-45ca-9170-f3fbafcaaa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a971913-c7a7-42ad-9f68-c33b567ac67c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[0;32m      4\u001b[0m modelxgb\u001b[38;5;241m=\u001b[39mxgb\u001b[38;5;241m.\u001b[39mXGBClassifier(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m modelxgb\u001b[38;5;241m.\u001b[39mfit(train_features, train_labels)\n\u001b[0;32m      7\u001b[0m predxgb \u001b[38;5;241m=\u001b[39m modelxgb\u001b[38;5;241m.\u001b[39mpredict(test_features)\n\u001b[0;32m      8\u001b[0m predxgb\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_features' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"#**XGBoost**\"\"\"\n",
    "\n",
    "import xgboost as xgb\n",
    "modelxgb=xgb.XGBClassifier(random_state=1,learning_rate=0.01)\n",
    "modelxgb.fit(train_features, train_labels)\n",
    "\n",
    "predxgb = modelxgb.predict(test_features)\n",
    "predxgb\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(test_labels,predxgb))\n",
    "print(confusion_matrix(test_labels, predxgb))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(test_labels, predxgb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4f64402e-1d81-4eb3-bcac-47e07ef65d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.80      0.82       852\n",
      "           1       0.85      0.88      0.86      1068\n",
      "\n",
      "    accuracy                           0.85      1920\n",
      "   macro avg       0.85      0.84      0.84      1920\n",
      "weighted avg       0.85      0.85      0.85      1920\n",
      "\n",
      "[[684 168]\n",
      " [127 941]]\n",
      "0.8463541666666666\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    duplicate       0.84      0.80      0.82       852\n",
      "non duplicate       0.85      0.88      0.86      1068\n",
      "\n",
      "     accuracy                           0.85      1920\n",
      "    macro avg       0.85      0.84      0.84      1920\n",
      " weighted avg       0.85      0.85      0.85      1920\n",
      "\n",
      "[[684 168]\n",
      " [127 941]]\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 假设 train_labels 和 test_labels 是字符串类型的标签\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "# 创建 XGBoost 模型\n",
    "modelxgb = xgb.XGBClassifier(random_state=1, learning_rate=0.01)\n",
    "modelxgb.fit(train_features, train_labels_encoded)  # 使用数值型标签训练模型\n",
    "\n",
    "# 进行预测\n",
    "predxgb = modelxgb.predict(test_features)\n",
    "\n",
    "# 将预测结果转换回原始标签\n",
    "predxgb_labels = label_encoder.inverse_transform(predxgb)\n",
    "\n",
    "# 评估模型性能\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(classification_report(test_labels_encoded, predxgb))  # 使用数值型标签评估\n",
    "print(confusion_matrix(test_labels_encoded, predxgb))  # 使用数值型标签评估\n",
    "print(accuracy_score(test_labels_encoded, predxgb))  # 使用数值型标签评估\n",
    "\n",
    "# 如果需要查看原始标签的评估结果\n",
    "print(classification_report(test_labels, predxgb_labels))  # 使用原始标签评估\n",
    "print(confusion_matrix(test_labels, predxgb_labels))  # 使用原始标签评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f2d82ea7-fc8d-4127-ac3f-94e8a364780a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#**KNN**\"\"\"\n",
    "\n",
    "#import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Setup arrays to store training and test accuracies\n",
    "neighbors = np.arange(1,9)\n",
    "train_accuracy =np.empty(len(neighbors))\n",
    "test_accuracy = np.empty(len(neighbors))\n",
    "\n",
    "for i,k in enumerate(neighbors):\n",
    "    #Setup a knn classifier with k neighbors\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    \n",
    "    #Fit the model\n",
    "    knn.fit(train_features, train_labels)\n",
    "    \n",
    "    #Compute accuracy on the training set\n",
    "    train_accuracy[i] = knn.score(train_features, train_labels)\n",
    "    \n",
    "    #Compute accuracy on the test set\n",
    "    test_accuracy[i] = knn.score(test_features, test_labels)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2066c073-3338-4351-a014-9a6c06730a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[703 149]\n",
      " [156 912]]\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    duplicate       0.82      0.83      0.82       852\n",
      "non duplicate       0.86      0.85      0.86      1068\n",
      "\n",
      "     accuracy                           0.84      1920\n",
      "    macro avg       0.84      0.84      0.84      1920\n",
      " weighted avg       0.84      0.84      0.84      1920\n",
      "\n",
      "0.8411458333333334\n"
     ]
    }
   ],
   "source": [
    "#Fit the model\n",
    "knn.fit(train_features,train_labels)\n",
    "\n",
    "knn.score(test_features,test_labels)\n",
    "\n",
    "y_pred = knn.predict(test_features)\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "y_pred\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(test_labels,y_pred))\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_labels,y_pred))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c7cdd5-ab3c-4050-b1a8-1bcbdabcae0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
